{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.578394Z",
     "start_time": "2021-02-26T20:47:37.060147Z"
    },
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "import jupyter_manim\n",
    "from manimlib.imports import *\n",
    "import pprint as pp\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_addons as tfa\n",
    "import os, datetime, pytz\n",
    "import tensorboard as tb\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train the following classifier using deep learning:\n",
    "1. Handwrite a digit from 0, ..., 9.\n",
    "1. Click predict to see if the app can recognize the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.586322Z",
     "start_time": "2021-02-26T20:47:39.580182Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.cs.cityu.edu.hk/~ccha23/mnist/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd00863eb50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://www.cs.cityu.edu.hk/~ccha23/mnist/\", width=805, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is deep learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is a technique of training a neural network with many layers of computational units called neurons. The following videos showcase some interesting applications of the technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.597013Z",
     "start_time": "2021-02-26T20:47:39.588214Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://slides.com/ccha23/dl_intro/embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd00863ee90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://slides.com/ccha23/dl_intro/embed\", width=805, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to train a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the training process in the following application, which trains a neural network that predicts the color of a given point when given its coordinate $(x_1,x_2)$.\n",
    "- Choose a data set from the `DATA` column.\n",
    "- Click the `play` button to start training the network. \n",
    "- `Epoch` is the number of times a neural network is trained iteratively using the data selected.\n",
    "- In the `OUTPUT` column, \n",
    "  - points in the blue region are predicted blue, and\n",
    "  - points in the orange region are predicted orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.607371Z",
     "start_time": "2021-02-26T20:47:39.598484Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.cs.cityu.edu.hk/~ccha23/playground\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd00af74550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://www.cs.cityu.edu.hk/~ccha23/playground\", width=900, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above app is a slight modification of the open source project [Tensorflow Playground](https://playground.tensorflow.org) with the additional features that:\n",
    "- You can save your configuration to the browser session by clicking the button `Save to browser session`. If you reopen the browser, it will load your previously saved configuration automatically.\n",
    "- You can reset the configuration by clicking the `reset` button.\n",
    "- The last button copies the permalink to your configuration to the clipboard. You can save multiple configurations and share them by keeping the permalinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the following uses the permalink to initialize the simplest neural network for the linearly separable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.617843Z",
     "start_time": "2021-02-26T20:47:39.609112Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.cs.cityu.edu.hk/~ccha23/playground/#activation=linear&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.15891&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcf11d51b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://www.cs.cityu.edu.hk/~ccha23/playground/#activation=linear&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.15891&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\", width=900, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Try to tune the parameters to classify the Spiral dataset until the testing loss reaches 0.02. Include a screenshot below and give the permalink to your configuration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3dd588956b73e36e44b25bf6de9e68c",
     "grade": true,
     "grade_id": "spiral",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also [fork and modify the code on GitHub](https://github.com/tensorflow/tensorflow/pulls) to add new features, e.g., to customize the datasets and store the trained neural network. However, since the visualization is limited 2D, it is difficult to extend the app for higher-dimensional dataset with multiple class values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, it is possible to train a practical neural network without any coding, by using a service called the [Teachable Machine](https://teachablemachine.withgoogle.com/). E.g., you may follow the interactive slides below to learn to train a machine that recognizes musical notes.  \n",
    "(Click the `play` button at the bottom of the slides to start the presentation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.629793Z",
     "start_time": "2021-02-26T20:47:39.619599Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.cs.cityu.edu.hk/~ccha23/tm/slides.html\" width=\"805\" height=\"450\" allow=\"microphone\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.cs.cityu.edu.hk/~ccha23/tm/slides.html\" width=\"805\" height=\"450\" allow=\"microphone\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Use [Teachable Machine](https://teachablemachine.withgoogle.com/) to train your machine. Explain what your machine does and include a link to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82802de9ab93d32a744131a48261b640",
     "grade": true,
     "grade_id": "tm",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is trained using an iterative algorithm called the *stochastic gradient descent*, which requires some background on vector calculus and probability theory. The lecture series below explain the idea nicely with mathematical animations made using the python library [mathematical animation engine (`manim`)](https://github.com/3b1b/manim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.640433Z",
     "start_time": "2021-02-26T20:47:39.632468Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.youtube.com/embed/videoseries?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcf11ced890>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://www.youtube.com/embed/videoseries?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\", width=805, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the mean-squared error for the loss function, we will consider the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss, which is more suitable for training a neural network classifier. We will have a glimpse of the information theory involved.\n",
    "$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to know about vector calculus?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help explain the theory, we will use some notations from vector linear algebra and probability theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Vectors](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)) in lowercase boldface font: \n",
    "\n",
    "$$\\M{x}=\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\end{bmatrix}$$\n",
    "\n",
    "- [Matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)) in uppercase boldface font:\n",
    "\n",
    "$$\\M{W}=\\begin{bmatrix}w_{11} & w_{12} & \\cdots \\\\\n",
    "w_{21} & \\ddots &  \\\\\n",
    "\\vdots &  &  \\end{bmatrix}$$\n",
    "\n",
    "- [Matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication):\n",
    "\n",
    "$$\\M{W}\\M{x} = \\begin{bmatrix}w_{11} & w_{12} & \\cdots \\\\\n",
    "w_{21} & \\ddots &  \\\\\n",
    "\\vdots &  &  \\end{bmatrix}\n",
    "\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}w_{11}x_1 + w_{12}x_2 + \\cdots \\\\ w_{21}x_1+\\cdots \\\\ \\vdots \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to know about Probability Theory?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Random variables](https://en.wikipedia.org/wiki/Random_variable) in sanserif font:  \n",
    "\n",
    "  $$\\underbrace{\\R{y}}_{\\text{random variable}}, \\underbrace{\\RM{x}=\\begin{bmatrix}\\R{x}_1 \\\\  \\R{x}_2 \\\\  \\vdots \\end{bmatrix}}_{\\text{random vector}}$$\n",
    "\n",
    "- [Support sets](https://en.wikipedia.org/wiki/Support_(mathematics)) in calligraphic font:  \n",
    "\n",
    "$$\\underbrace{\\mc{Y}=\\Set{0,1,\\dots,k-1}}_{\\text{finite set}}, \\underbrace{\\mc{X}=\\mc{X}_1\\times \\mc{X}_2 \\times \\cdots}_{\\text{product space}}$$\n",
    "\n",
    "- [Joint distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution#Mixed_case):  \n",
    "\n",
    "$$p_{\\RM{x}\\R{y}}(\\M{x},y)= \\underbrace{p_{\\R{y}|\\RM{x}}(y|\\M{x})}_{\\underbrace{\\Pr}_{\\text{probability measure}\\kern-3em}\\Set{\\R{y}=y|\\RM{x}=\\M{x}}} \\cdot \\underbrace{p_{\\RM{x}}(\\M{x})}_{(\\underbrace{\\partial_{x_1}}_{\\text{partial derivative w.r.t. $x_1$}\\kern-5em} \\partial_{x_2}\\cdots)\\Pr\\Set{\\RM{x} \\leq \\M{x}}\\kern-4em}\\kern1em \\text{where}$$  \n",
    "  - $p_{\\R{y}|\\RM{x}}(y|\\M{x})$ is the *probability mass function [(pmf)](https://en.wikipedia.org/wiki/Probability_mass_function)* of $\\R{y}=y\\in \\mc{Y}$ [conditioned](https://en.wikipedia.org/wiki/Conditional_probability_distribution) on $\\RM{x}=\\M{x}\\in \\mc{X}$, and\n",
    "  - $p_{\\RM{x}}(\\M{x})$ is the *(multivariate) probability density function [(pdf)](https://en.wikipedia.org/wiki/Probability_density_function#Densities_associated_with_multiple_variables)* of $\\RM{x}=\\M{x}\\in \\mc{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For any function $g$ of $(\\RM{x},y)$, the expectations are:  \n",
    "  \n",
    "\\begin{align}\n",
    "\\E[g(\\RM{x},\\R{y})|\\RM{x}]&=\\sum_{y\\in \\mc{Y}} g(\\RM{x},y)\\cdot p_{\\R{y}|\\RM{x}}(y|\\RM{x})\\tag{conditional expectation}\n",
    "\\\\\n",
    "\\E[g(\\RM{x},\\R{y})] &=\\int_{\\mc{X}} \\underbrace{\\sum_{y\\in \\mc{Y}} g(\\RM{x},y)\\cdot \\underbrace{p_{\\RM{x},\\R{y}}(\\M{x},y)}_{p_{\\R{y}|\\RM{x}}(y|\\M{x}) p_{\\R{x}}(\\M{x})}\\kern-1.7em}_{\\E[g(\\RM{x},\\R{y})|\\RM{x}]}\\kern1.4em\\,\\d \\M{x} \\tag{expectation}\\\\\n",
    "&= \\E[\\E[g(\\RM{x},\\R{y})|\\RM{x}]] \\tag{iterated expectation}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are some manims created to introduce the above notions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.650474Z",
     "start_time": "2021-02-26T20:47:39.643057Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Notation:</b><br>\n",
       "<video width=\"805\" height=\"450\" controls>\n",
       "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/Notation.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<b>Notation:</b><br>\n",
    "<video width=\"805\" height=\"450\" controls>\n",
    "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/Notation.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.660957Z",
     "start_time": "2021-02-26T20:47:39.652299Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Distribution:</b><br>\n",
       "<video width=\"805\" height=\"450\" controls>\n",
       "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/Distribution.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<b>Distribution:</b><br>\n",
    "<video width=\"805\" height=\"450\" controls>\n",
    "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/Distribution.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.671982Z",
     "start_time": "2021-02-26T20:47:39.662855Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Expectation:<br>\n",
       "<video width=\"805\" height=\"450\" controls>\n",
       "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/Expectation.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<b>Expectation:<br>\n",
    "<video width=\"805\" height=\"450\" controls>\n",
    "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/Expectation.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T02:23:25.150612Z",
     "start_time": "2021-02-22T02:23:25.143155Z"
    }
   },
   "source": [
    "You may also create your own animations with `manim` in the jupyter notebook using `jupyter_manim` and `manimlib` as described [here](https://ccha23.github.io/CS1302ICP/Lecture5/Objects.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:40.757578Z",
     "start_time": "2021-02-26T11:10:40.748940Z"
    }
   },
   "outputs": [],
   "source": [
    "import jupyter_manim\n",
    "from manimlib.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Run the following cell and see the effect when changing\n",
    "\n",
    "- Mobjects: `TextMobject('Hello, World!')` to `TexMobject(r'E=mc^2')` or `Circle()` or `Square()`.\n",
    "- Animation objects: `Write` to `FadeIn` or `GrowFromCenter`.\n",
    "\n",
    "You may take a look at the documentation [here](https://docs.manim.community/en/v0.2.0/index.html) and a more detailed [tutorial](https://talkingphysics.wordpress.com/2019/01/08/getting-started-animating-with-manim-and-python-3-7/) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:41.383256Z",
     "start_time": "2021-02-26T11:10:40.759243Z"
    }
   },
   "outputs": [],
   "source": [
    "%%manim HelloWorld -l\n",
    "class HelloWorld(Scene):\n",
    "    def construct(self):\n",
    "        self.play(Write(TextMobject('Hello, World!')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network learns from many examples collected together as a *dataset*. For instance, the [MNIST (Modified National Institute of Standards and Technology)](https://en.wikipedia.org/wiki/MNIST_database) dataset consists of labeled handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a title=\"Josef Steppan, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an example in a dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset is a sequence \n",
    "\n",
    "\\begin{align}\n",
    "(\\RM{x}_1,\\R{y}_1),(\\RM{x}_2,\\R{y}_2), \\dots\\tag{dataset}\n",
    "\\end{align}\n",
    "\n",
    "of *tuples/instances* $(\\RM{x}_i,\\R{y}_i)$, each of which consists of\n",
    "- an *input feature vector* $\\RM{x}_i$ such as an image of a handwritten digit and\n",
    "- a *label* $\\R{y}_i$ such as the digit type of the handwritten digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to learn from the dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problem in Machine Learning is *classification*: The goal is to train a *classifier* that predicts a label $\\R{y}$ for an input feature $\\RM{x}$: \n",
    "-  A hard-decision classifier is a function $f:\\mc{X}\\to \\mc{Y}$ such that\n",
    "$f(\\RM{x})$ predicts $\\R{y}$.\n",
    "- A probabilistic classifier is a conditional distribution $q_{\\R{y}|\\RM{x}}$ that estimates $p_{\\R{y}|\\RM{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST, the goal is to classify the digit type of a handwritten digit.  When given a handwritten digit,\n",
    "- a hard-decision classifier returns a digit type, and\n",
    "- a probabilistic classifier returns a distribution of the digit types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why consider a probabilistic classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often train a neural network as a probabilistic classifer because:\n",
    "- A probabilistic classifer is more general and can give a hard decision as well   \n",
    "\n",
    "  $$f(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} q_{\\R{y}|\\RM{x}}(y|\\RM{x})$$ \n",
    "  by returning the estimated most likely digit type. \n",
    "  \n",
    "- A neural network can model the distribution $p_{\\R{y}|\\RM{x}}(\\cdot|\\RM{x})$ better than $\\R{y}$ because its output is continous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why can we learn from examples?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem to be meaningful, $(\\RM{x},\\R{y})$ is assumed to be random with some unknown joint distribution $p_{\\RM{x},\\R{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we always had $\\R{y}=y$ instead, then a perfect classifier can just return $y$ without even looking at $\\RM{x}$.\n",
    "- If $p_{\\RM{x},\\R{y}}$ were known instead, then $p_{\\R{y}|\\RM{x}}$ would also be known and therefore needed not be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples are often collected randomly and independently from a population, i.e., as [i.i.d. samples](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) of $(\\RM{x},\\R{y})$:\n",
    "\n",
    "$$(\\RM{x}_1,\\R{y}_1), (\\RM{x}_2,\\R{y}_2), \\dots\\sim_{\\text{i.i.d.}} p_{\\RM{x},\\R{y}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If all the examples were the same instead, the examples could not show the pattern of how $\\R{y}$ depended on $\\RM{x}$.\n",
    "- The observed distribution of i.i.d. samples converge to the unknown distribution by the [law of large number](https://en.wikipedia.org/wiki/Law_of_large_numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to determine if a classifier is good?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we desire a classifier with the maximum accuracy in predicting $\\R{y}$ but doing so is [computationally too difficult](https://en.wikipedia.org/wiki/Loss_functions_for_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we regard a classification algorithm to be reasonably good if it can achieve the maximum possible accuracy as the number of training samples goes to $\\infty$. This is more formally stated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** A probabilistic classifier for the input feature $\\RM{x}$ and label $\\R{y}$ with unknown joint distribution  is a conditional pmf\n",
    "\n",
    "$$\n",
    "\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})\\qquad \\text{for }\\M{x}\\in \\mc{X}, y\\in \\mc{Y}\n",
    "$$\n",
    "\n",
    "defined as a function of the i.i.d. samples\n",
    "\n",
    "$$\\{(\\RM{x}_i,\\R{y}_i)\\}_{i=1}^N$$\n",
    "\n",
    "of $(\\RM{x},\\R{y})$ (but independent of $(\\RM{x},\\R{y})$). \n",
    "The classifier is said to be a *consistent* estimate (of $p_{\\R{y}|\\M{x}}$) if\n",
    "\n",
    "\\begin{align}\n",
    "\\lim_{N\\to \\infty} \\Pr\\Set{\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x})\\text{ for all } y\\in \\mc{Y}}=1,\\tag{consistency}\n",
    "\\end{align}\n",
    "\n",
    "namely, $\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})$ converges almost surely (a.s.) to $p_{\\R{y}|\\RM{x}}$. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A consistent probabilistic classifier gives rise to an asymptotically optimal hard-decision classifier that achieves the maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition** If for some $\\epsilon\\geq 0$ that\n",
    "\n",
    "$$\\Pr\\Set{\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x}) \\text{ for all } y\\in \\mc{Y}}=1-\\epsilon,$$\n",
    "\n",
    "the (hard-decision) classifier\n",
    "\n",
    "\\begin{align}\\R{f}(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} \\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})\\tag{hard decision}\\end{align}\n",
    "\n",
    "achieves an accuracy\n",
    "\n",
    "\\begin{align}\n",
    "\\sup_{f:\\mc{X}\\to \\mc{Y}} \\Pr(\\R{y}= f(\\RM{x})) &\\geq \\E\\left[\\max_{y\\in \\mc{Y}} p_{\\R{y}|\\M{x}}(y|\\RM{x})\\right] - \\epsilon.\\tag{accuracy lower bound}\n",
    "\\end{align}\n",
    "\n",
    "where the expectation is the maximum possible accuracy. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof:* For any classifier $f$,\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\Pr(\\R{y}= f(\\RM{x}))\n",
    "&= \\E[p_{\\R{y}|\\M{x}}(f(\\RM{x})|\\RM{x})] \\\\\n",
    "&\\leq \\E\\left[\\max_{y\\in \\mc{Y}} p_{\\R{y}|\\M{x}}(y|\\RM{x})\\right]\\end{align*}\n",
    "$$\n",
    "\n",
    "where the last inequality is achievable with equality with the hard-decision classifier and $\\R{q}$ replaced by $p$. This implies the desired accuracy lower bound for the case $\\epsilon=0$. The more general case with $\\epsilon\\geq 0$ can be derived similarly. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we obtain a consistent classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a neural network to minimize certain *loss*. A common loss function for classification uses the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) measure in information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical underpinning is the following identity that relates three information quantities:\n",
    "\n",
    "\\begin{align}\n",
    "\\overbrace{\\E\\left[\\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{ \\text{Cross entropy}\\\\ H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=} &\\equiv \\overbrace{\\E\\left[\\log \\frac{1}{p_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{\\text{Conditional entropy}\\\\ H(\\R{y}|\\RM{x}):=} + \\overbrace{\\E\\left[\\log \\frac{p_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right].}^{\\text{Divergence}\\\\ D(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=}\\tag{identity}\n",
    "\\end{align}\n",
    "\n",
    "The identity can be proved quite easily using the linearity of expectation \n",
    "\n",
    "$$ \\E[\\R{u}+\\R{v}]=\\E[\\R{u}]+\\E[\\R{v}],$$\n",
    "\n",
    "and a property of logarithm that \n",
    "\n",
    "$$\\log uv = \\log u+ \\log v.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.683597Z",
     "start_time": "2021-02-26T20:47:39.674136Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Information Identity</b><br>\n",
       "<video width=\"805\" height=\"450\" controls>\n",
       "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/InformationIdentity.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<b>Information Identity</b><br>\n",
    "<video width=\"805\" height=\"450\" controls>\n",
    "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/InformationIdentity.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition** With $q_{\\R{y}|\\RM{x}}(y|\\M{x})$ being a valid pmf of a random variable taking values from $\\mc{Y}$ conditioned on a random variable taking values from $\\mc{X}$, \n",
    "\n",
    "\\begin{align}\n",
    "\\min_{q_{\\R{y}|\\RM{x}}} H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}})\n",
    "&= H(\\R{y}|\\RM{x})\n",
    "\\end{align}\n",
    "\n",
    "and the optimal solution equals $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$. $\\square$ \n",
    "\n",
    "Hence, a neural network that minimizes the cross entropy equals $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$ and any possible input image $\\RM{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof:* It suffices to show that \n",
    "\n",
    "\\begin{align}\n",
    "D(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}})\\geq 0 \\tag{positivity of divergence}\n",
    "\\end{align}\n",
    "\n",
    "with equality iff $q_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. This, in turn, can be proved using the [log-sum inequality](https://en.wikipedia.org/wiki/Log_sum_inequality):\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i} a_i \\log\\left(\\frac{a_i}{b_i}\\right) \\geq (\\textstyle \\sum_{i} a_i) \\log\\left(\\frac{\\sum_{i} a_i}{\\sum_{i} b_i}\\right)\\tag{log-sum inequality}\n",
    "\\end{align} \n",
    "\n",
    "for any sequences $\\{a_i\\}$, $\\{b_i\\}$, and $\\{c_i\\}$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the iris dataset, the MNIST dataset can be obtained in many ways due to its popularity in image recognition. For instance, one may use `tensorflow.keras.datasets.mnist.load_data` to load the data as tuples/arrays and convert it to `DataFrame`. However, training a neural network often requires a lot of data and computational power. It may be inefficient or impossible to load all data into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way is to use the package [`tensorflow_datasets`](https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html), which lazily load the dataset and prepare the data as [`Tensor`s](https://www.tensorflow.org/guide/tensor), which can be operated faster by GPU or TPU instead of CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:41.631041Z",
     "start_time": "2021-02-26T11:10:41.391510Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # give a shorter name tfds for convenience\n",
    "import os\n",
    "\n",
    "user_home = os.getenv(\"HOME\")  # get user home directory\n",
    "data_dir = os.path.join(user_home, \"data\")  # download folder for data\n",
    "\n",
    "ds, ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    data_dir=data_dir,  # download location\n",
    "    as_supervised=True,  # separate input features and label\n",
    "    with_info=True,  # return information of the dataset\n",
    ")\n",
    "\n",
    "# print information related to loading the dataset\n",
    "import pprint as pp\n",
    "print('-' * 79)\n",
    "print(f'Data downloaded to {data_dir}')\n",
    "print(f'Data to be loaded in memory:')\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `tfds.load` download the data to `data_dir` and prepare it for loading using variable `ds`. In particular, the dataset is split into \n",
    "- a training set `ds[\"train\"]` and\n",
    "- a test set `ds[\"test\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tfds.load?` shows more information about the function. E.g., we can control the split ratio using the argument [`split`](https://www.tensorflow.org/datasets/splits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why split the data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is used to evaluate the performance of a neural network trained using the training set (separate from the test set).\n",
    "\n",
    "The purpose of separating the test set from the training set is to avoid *overly-optimistic* performance estimate. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the final exam questions (test set) are the same as the previous homework questions (training set). \n",
    "- Students may get a high exam score simply by studying the model answers to the homework instead of understanding entire subject.\n",
    "- The exam score is therefore an overly-optimistic estimate of the students' understanding of the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Assign to `train_size` and `test_size` the numbers of instances in the training set and test set respectively.\n",
    "\n",
    "*Hint: Both the training and test sets are loaded as [`Dataset` objects](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Since the loading is lazy, i.e., the data is not yet in memory, we cannot count the number of instances directly. Instead, we obtain such information from `ds_info`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:41.636861Z",
     "start_time": "2021-02-26T11:10:41.632751Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fe707e215aaa5fd29874fbde3610c8c",
     "grade": false,
     "grade_id": "sizes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:41.648365Z",
     "start_time": "2021-02-26T11:10:41.638370Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4439f3ae0583d5a1bc5599ca7a6a8fea",
     "grade": true,
     "grade_id": "test-sizes",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training set is often much larger than the test set especially for deep learning because \n",
    "- training a neural network requires many examples but\n",
    "- estimating its performance does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following retrieves an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:41.749307Z",
     "start_time": "2021-02-26T11:10:41.649939Z"
    }
   },
   "outputs": [],
   "source": [
    "for image, label in ds[\"train\"].take(1):\n",
    "    print(\n",
    "        f'image dtype: {type(image)} shape: {image.shape} element dtype: {image.dtype}'\n",
    "    )\n",
    "    print(f'label dtype: {label.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for loop above takes one example from `ds[\"train\"]` using the method [`take`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) and print its data types. \n",
    "- The handwritten digit is represented by a 28x28x1 [`EagerTensor`](https://www.tensorflow.org/guide/eager), which is essentially a 2D array of bytes (8-bit unsigned integers `uint8`). \n",
    "- The digit type is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots the image using the `imshow` function from `matplotlib.pyplot`. We set the parameter `cmap` to `gray_r` so the color is darker if the pixel value is larger. The slice operator `[:,:,0]` for the image reshaped the numpy array from 3 dimensions to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:10:51.994417Z",
     "start_time": "2021-02-26T11:10:41.751213Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mnist_image(example, ax=None, pixel_format=None):\n",
    "    (image, label) = example\n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    ax.title.set_text(label.numpy())  # show digit type as plot title\n",
    "    ax.imshow(image[:, :, 0], cmap=\"gray_r\")  # show image\n",
    "    # Major ticks\n",
    "    ax.set_xticks(np.arange(0, 28, 3))\n",
    "    ax.set_yticks(np.arange(0, 28, 3))\n",
    "    # Minor ticks\n",
    "    ax.set_xticks(np.arange(-.5, 28, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 28, 1), minor=True)\n",
    "    if pixel_format is not None:\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    pixel_format.format(image[i, j,\n",
    "                                              0].numpy()),  # show pixel value\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    color='white',\n",
    "                    fontweight='bold',\n",
    "                    fontsize='small')\n",
    "        ax.grid(color='lightblue', linestyle='-', linewidth=1, which='minor')\n",
    "        ax.set_xlabel('2nd dimension')\n",
    "        ax.set_ylabel('1st dimension')\n",
    "        ax.title.set_text('Image with label ' + ax.title.get_text())\n",
    "\n",
    "\n",
    "if input('Execute? [Y/n]').lower != 'n':\n",
    "    plt.figure(figsize=(11, 11), dpi=80)\n",
    "    for example in ds[\"train\"].take(1):\n",
    "        plot_mnist_image(example, pixel_format='{}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "360389373a01a5b93a9b30abfa838341",
     "grade": false,
     "grade_id": "check-plot_mnist_image_matrix",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Exercise** Complete the following code to generate a matrix plot of the first 50 examples from the training sets.  \n",
    "The parameter `nrows` and `ncols` specify the number of rows and columns respectively. You code may look like\n",
    "```Python\n",
    "...\n",
    "        for ax, example in zip(axes.flat, ds[\"train\"].____(nrows * ncols)):\n",
    "            plot_mnist_image(_______, ax)\n",
    "            ax.axes.xaxis.set_visible(False)\n",
    "            ax.axes.yaxis.set_visible(False)\n",
    "...\n",
    "```\n",
    "and the output image should look like\n",
    "![mnist_examples](mnist_examples.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:00.984728Z",
     "start_time": "2021-02-26T11:10:52.000396Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55e97876ef81ed5c47d056ac1e5d2cc9",
     "grade": false,
     "grade_id": "plot_mnist_image_matrix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower != 'n':\n",
    "    def plot_mnist_image_matrix(ds, nrows=5, ncols=10):\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        fig.tight_layout()  # adjust spacing between subplots automatically\n",
    "        return fig, axes\n",
    "\n",
    "\n",
    "    fig, axes = plot_mnist_image_matrix(ds, nrows=5)\n",
    "    fig.set_figwidth(9)\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_dpi(80)\n",
    "    # plt.savefig('mnist_examples.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [`tensorflow`](https://www.tensorflow.org/) library to process the data and train the neural network. (Another popular library is [PyTorch](https://pytorch.org/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:00.988700Z",
     "start_time": "2021-02-26T11:11:00.986341Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf  # explicitly use tensorflow version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel is stored as an integer from $\\Set{0,\\dots,255}$ ($2^8$ possible values). However, for computations by the neural network, we need to convert it to a floating point number. We will also normalize each pixel value to be within the unit interval $[0,1]$:\n",
    "\n",
    "\\begin{align} \n",
    "v \\mapsto \\frac{v - v_{\\min}}{v_{\\max} - v_{\\min}} = \\frac{v}{255}\\tag{min-max normalization}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Using the function `map`, normalize each element of an image to the unit interval $[0,1]$ after converting them to `tf.float32` using [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast).\n",
    "\n",
    "*Hint:* The normalization factor is NOT 256. You code may look like\n",
    "```Python\n",
    "...\n",
    "        ds_n[part] = ds[part].map(\n",
    "                    lambda image, label: (_____(image, _____) / ___, label),\n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "...\n",
    "```\n",
    "and the output image should look like\n",
    "![mnist_example](mnist_example_normalized.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.221285Z",
     "start_time": "2021-02-26T11:11:00.990085Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb51b68000200c95d62e83c8acff9064",
     "grade": false,
     "grade_id": "normalize",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_mnist(ds):\n",
    "    \"\"\"\n",
    "  Returns:\n",
    "  MNIST Dataset with image pixel values normalized to float32 in [0,1].\n",
    "  \"\"\"\n",
    "    ds_n = dict.fromkeys(ds.keys())  # initialize the normalized dataset\n",
    "    for part in ds.keys():\n",
    "        # normalize pixel values to [0,1]\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return ds_n\n",
    "\n",
    "\n",
    "ds_n = normalize_mnist(ds)\n",
    "pp.pprint(ds_n)\n",
    "plt.figure(figsize=(11, 11), dpi=80)\n",
    "for example in ds_n[\"train\"].take(1):\n",
    "    plot_mnist_image(example,\n",
    "                     pixel_format='{:.2f}')  # show pixel value to 2 d.p.s\n",
    "plt.savefig('mnist_example_normalized.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.369510Z",
     "start_time": "2021-02-26T11:11:07.223039Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5de663ce934246c44d594b613ff472f8",
     "grade": true,
     "grade_id": "test-normalize",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, the training of a neural network uses *stochastic gradient descent* which\n",
    "- divides the training into many steps where\n",
    "- each step uses a *randomly* selected minibatch of samples \n",
    "- to improve the neural network *bit-by-bit*. \n",
    "\n",
    "The following code specifies the batch size and enables caching and prefetching to reduce the latency in loading examples repeatedly for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.381922Z",
     "start_time": "2021-02-26T11:11:07.373293Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_mnist(ds_n):\n",
    "    ds_b = dict.fromkeys(ds_n.keys())  # initialize the batched dataset\n",
    "    for part in ds_n.keys():\n",
    "        ds_b[part] = (\n",
    "            ds_n[part].batch(\n",
    "                128)  # Use a minibatch of examples for each training step\n",
    "            .shuffle(\n",
    "                ds_info.splits[part].num_examples,\n",
    "                reshuffle_each_iteration=True)  # shuffle data for each epoch\n",
    "            .cache()  # cache current elements \n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )  # preload subsequent elements\n",
    "    return ds_b\n",
    "\n",
    "\n",
    "ds_b = batch_mnist(ds_n)\n",
    "pp.pprint(ds_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** The output to the above cell should look like\n",
    "```Python\n",
    "{'test': <PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>,\n",
    " 'train': <PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>}\n",
    "```\n",
    "with a new first dimension of unknown size `None`. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e776f9d0a83c8d12eb4fe6c9aff182e",
     "grade": true,
     "grade_id": "batch",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the neural network computes an estimate $q_{\\R{y}|\\RM{x}}(y|\\M{x})$ of the unknown probability $p_{\\R{y}|\\RM{x}}(y|\\M{x})$ for any image $\\M{x}\\in \\mc{X}$ and label $y\\in \\mc{Y}$. The computation is organized into layers of computation units called the *neurons*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\ell\\in \\{0,\\dots,L\\}$ and integer $L\\geq 1$, let \n",
    "- $\\M{a}^{(\\ell)}$ be the output of the $\\ell$-th layer of the neural network, and\n",
    "- $a^{(\\ell)}_i$ be the $i$-th element of $\\M{a}^{(\\ell)}$. The element is computed from the output $\\M{a}^{(\\ell-1)}$ of its previous layer except for $\\ell=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $0$-th layer is called the *input layer* while the $L$-th layer is called the *output layer*. All other layers are called the *hidden layers*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic neural network architecture computes $q_{\\R{y}|\\RM{x}}(y|\\M{x})$ as\n",
    "\n",
    "\\begin{align}\n",
    "[q_{\\R{y}|\\RM{x}}(y|\\M{x})]_{y\\in \\mc{Y}} &:= \\M{a}^{(L)} \n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "\\M{a}^{(\\ell)}&:=\\begin{cases}\n",
    "\\M{x} & \\ell=0\\\\\n",
    "\\sigma^{(\\ell)}(\\overbrace{\\M{W}^{(\\ell)}\\M{a}^{(\\ell-1)}+\\M{b}^{(\\ell)}}^{\\RM{z}^{(\\ell)}:=})& \\ell>0;\n",
    "\\end{cases}\\tag{net}\n",
    "\\end{align}\n",
    "\n",
    "- $\\M{W}^{(\\ell)}$ is a matrix of weights;\n",
    "- $\\M{b}^{(\\ell)}$ is a vector called bias; and\n",
    "- $\\sigma^{(\\ell)}$ is a reveal-valued function called the *activation function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure $\\M{a}^{(L)}$ is a valid probability vector, the soft-max activation function is often used for the last layer:\n",
    "\n",
    "$$ \\sigma^{(L)}\\left(\\left[\\begin{smallmatrix}z^{(\\ell)}_1 \\\\ \\vdots \\\\ z^{(\\ell)}_k\\end{smallmatrix}\\right]\\right) := \\frac{1}{\\sum_{i=1}^k e^{z^{(\\ell)}_i}} \\left[\\begin{smallmatrix}e^{z^{(\\ell)}_1} \\\\ \\vdots \\\\ e^{z^{(\\ell)}_k}\\end{smallmatrix}\\right] $$ \n",
    "\n",
    "where $k:=\\abs{\\mc{Y}}=10$ is the number of distinct class labels. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures $\\M{a}^{(L)}$ (the output of soft-max) is stochastic, i.e., \n",
    "\n",
    "$$\\sum_{i=1}^k a_i^{(L)}  = 1\\kern1em \\text{and} \\kern1em a_i^{(L)}\\geq 0\\qquad \\forall i\\in \\{1,\\dots,k\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions $\\sigma^{(\\ell)}$ for other layers $1\\leq \\ell<L$ is often the vectorized version of \n",
    "-  sigmoid:  \n",
    "\n",
    "  $$\\sigma_{\\text{sigmoid}}(z) = \\frac{1}{1+e^{-z}}$$\n",
    "-  rectified linear unit (ReLU): \n",
    "\n",
    "  $$ \\sigma_{\\text{ReLU}}(z) = \\max\\{0,z\\}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.695692Z",
     "start_time": "2021-02-26T20:47:39.686193Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Activation function:</b><br>\n",
       "<iframe width=\"805\" height=\"450\" src=\"https://www.youtube.com/embed/aircAruvnKk?start=649&end=695\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<b>Activation function:</b><br>\n",
    "<iframe width=\"805\" height=\"450\" src=\"https://www.youtube.com/embed/aircAruvnKk?start=649&end=695\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.737654Z",
     "start_time": "2021-02-26T11:11:07.397024Z"
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.max([np.zeros(z.shape), z], axis=0)\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.plot(z, ReLU(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r'ReLU: $\\max\\{0,z\\}$')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T10:40:54.078963Z",
     "start_time": "2021-02-22T10:40:54.074180Z"
    }
   },
   "source": [
    "**Exercise** Complete the vectorized function `sigmoid` using the vectorized exponentiation `np.exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.880124Z",
     "start_time": "2021-02-26T11:11:07.739435Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f299b139d55fccf5a920653eb24edf1",
     "grade": false,
     "grade_id": "sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "plt.plot(z, ReLU(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r'Sigmoid function: $\\frac{1}{1+e^{-z}}$')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.885243Z",
     "start_time": "2021-02-26T11:11:07.881980Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa581795d83f22d1d7242e5de68f5ab",
     "grade": true,
     "grade_id": "test-sigmoid",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following uses the [`keras`](https://keras.io/) library to define the basic neural network achitecture. `keras` runs on top of `tensorflow` and offers a higher-level abstraction to simplify the construction and training of a neural network. ([`tflearn`](https://github.com/tflearn/tflearn) is another library that provides a higher-level API for `tensorflow`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.936381Z",
     "start_time": "2021-02-26T11:11:07.886595Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    tf.keras.backend.clear_session() # clear keras cache. \n",
    "                        # See https://github.com/keras-team/keras/issues/7294\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
    "    ], 'Simple_sequential')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_simple_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above defines [a linear stack](https://www.tensorflow.org/api_docs/python/tf/keras/layers) of [fully-connected layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) after [flattening the input](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten). The method `summary` is useful for [debugging in Keras](https://keras.io/examples/keras_recipes/debugging_tips/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execise** Assign to `n_hidden_layers` the number of hidden layers for the above simple sequential model. \n",
    "\n",
    "*Hint:* The layer `Flatten` do not counts as a hidden layer since it simply reshape the input without using any trainable parameters. The output layer also do not count as a hidden layer since its output is the output of the neural network, not intermediate (hidden) values that require further processing by the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.941508Z",
     "start_time": "2021-02-26T11:11:07.938098Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ed972ecc58c3a66402e21da68bc57e",
     "grade": false,
     "grade_id": "simple-seq",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "n_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.952758Z",
     "start_time": "2021-02-26T11:11:07.942967Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a506eca5aeefcc6be556f35873038c4",
     "grade": true,
     "grade_id": "test-simple-seq",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a neural network that minimizes the cross entropy gives $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$ and any possible input image $\\RM{x}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{q_{\\R{y}|\\RM{x}}} \\overbrace{\\E\\left[\\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{ \\text{Cross entropy}\\\\ H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=}\n",
    "&= H(\\R{y}|\\RM{x})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy cannot be computed exactly without knowing the joint distribution $p_{\\RM{x}\\R{y}}$. Nevertheless, it can be estimated from a batch of i.i.d. samples $(\\RM{x}_{\\R{j}_i},\\R{y}_{\\R{j}_i})$ for $1\\leq i\\leq n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\R{L}(\\theta)&:=\\frac1n \\sum_{i=1}^n \\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}_{\\R{j}_i}|\\RM{x}_{\\R{j}_i})}\\tag{empirical loss}\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "$$\\theta := \\operatorname{flat}(\\M{W}^{(\\ell)},\\M{b}^{(\\ell)}\\mid 0\\leq \\ell \\leq L)$$\n",
    "\n",
    "is the vector of parameters of the neural network defined in (net)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini-batch [gradient descent algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is often used to reduce the loss. It iteratively updates/trains the neural network parameters:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta -s\\nabla \\R{L}(\\theta)$$\n",
    "\n",
    "by computing the gradient $\\nabla \\R{L}(\\theta)$ on a randomly selected minibatch of examples and choosing an appropriate learning rate $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the [lecture series on deep learning](https://www.youtube.com/embed/videoseries?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) in the introduction section:\n",
    "- The gradient can be computed systematically using a technique called *backpropagation* due to the structure of the neural network in (net).\n",
    "- The learning rate can affect the convergence rate of the loss to a local minima: \n",
    "    - $\\theta$ may overshoot its optimal value if $s$ is too large, and\n",
    "    - the convergence can be very slow if $\\theta$ is too small.\n",
    "\n",
    "A more advanced method called [Adam (Adaptive Momentum Estimation)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) can adaptively choose $s$ to speed up the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:47:39.704057Z",
     "start_time": "2021-02-26T20:47:39.698480Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<b>What is gradient descent?</b><br>\n",
       "<iframe width=\"805\" height=\"450\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=468&end=510\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n",
       "<div>\n",
       "<b>How to choose the step size?</b><br>\n",
       "<iframe width=\"805\" height=\"450\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=403&end=415\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div>\n",
    "<b>What is gradient descent?</b><br>\n",
    "<iframe width=\"805\" height=\"450\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=468&end=510\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "<div>\n",
    "<b>How to choose the step size?</b><br>\n",
    "<iframe width=\"805\" height=\"450\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=403&end=415\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [loss function](https://keras.io/api/losses/), gradient descent algorithm, and the performance metrics can be specified using the [`compile` method](https://keras.io/api/losses/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:07.990125Z",
     "start_time": "2021-02-26T11:11:07.967266Z"
    }
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "compile_model(model)\n",
    "model.loss, model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the neural network using the method [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) of the compiled model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:17.565196Z",
     "start_time": "2021-02-26T11:11:07.991800Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    model.fit(ds_b[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** By default, the neural network is trained for 1 epoch. What happens to the training accuracy if you rerun the above cell to train the model for another epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3359d5d7ef71d2a3d7de7c29c5b7f0a1",
     "grade": true,
     "grade_id": "retrain",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the parameter `epochs` to train the neural network for multiple epochs since it is quite unlikely to train a neural network well with just one epoch. To determine whether the neural network is well-trained (when to stop training), we should also use a separate validation set to evaluate the performance of the neural network. The validation set can be specified using the parameter `validation_set` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:21.677298Z",
     "start_time": "2021-02-26T11:11:17.568571Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    model.fit(ds_b[\"train\"], epochs=6, validation_data=ds_b[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Is the maximum validation accuracy `val_sparse_categorical_accuracy` (over different epoches) an unbiased estimate of the performance of deep learning for the given dataset? If not, how to avoid the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "336aded134c9923168791d9525d47565",
     "grade": true,
     "grade_id": "bias",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Is it a good idea to use cross-validation to evaluate the neural network? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce7ec97b4fd46cea7a8e95b5e19edc49",
     "grade": true,
     "grade_id": "cv",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call additional functions during training, we can add the functions to the `callbacks` parameter of the model `fit` method. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:25.675714Z",
     "start_time": "2021-02-26T11:11:21.680870Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm.keras\n",
    "\n",
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    model.fit(ds_b[\"train\"],\n",
    "              epochs=6,\n",
    "              validation_data=ds_b[\"test\"],\n",
    "              verbose=0,\n",
    "              callbacks=[tqdm.keras.TqdmCallback(verbose=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses [`tqdm.keras.TqdmCallback()`](https://tqdm.github.io/docs/keras/) to return a callback function that displays a graphical progress bar:\n",
    "- Setting `verbose=0` for the method `fit` disables the default text-based progress bar.\n",
    "- Setting `verbose=2` for the class `TqdmCallback` show and keep the progress bars for training each batch. Try changing `verbose` to other values to see different effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An import use of callback functions is to save the models and results during training for further analysis. We define the following function `train_model` for this purpose:\n",
    "- Take a look at the docstring to learn its basic usage, and then\n",
    "- learn the implementations in the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:25.692135Z",
     "start_time": "2021-02-26T11:11:25.678793Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, datetime, pytz\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                fit_params={},\n",
    "                log_root='.',\n",
    "                save_log_params=None,\n",
    "                save_model_params=None,\n",
    "                debug_params=None):\n",
    "    '''Train and test the model, and return the log directory path name.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    log_root (str): the root directory for creating log directory\n",
    "    \n",
    "    fit_params (dict): dictionary of parameters to pass to model.fit.\n",
    "    save_log_params (dict): dictionary of parameters to pass to \n",
    "        tf.keras.callbacks.TensorBoard to save the results for TensorBoard.\n",
    "        The default value None means no logging of the results.\n",
    "    save_model_params (dict): dictionary of parameters to pass to\n",
    "        tf.keras.callbacks.ModelCheckpoint to save the model to checkpoint \n",
    "        files. \n",
    "        The default value None means no saving of the models.\n",
    "    debug_params (dict): dictionary of parameters to pass to \n",
    "        tf.debugging.experimental.enable_dump_debug_info for debugger \n",
    "        v2 in tensorboard.\n",
    "        The default value None means no logging of the debug information.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    str: log directory path that points to a subfolder of log_root named \n",
    "        using the current time.\n",
    "    '''\n",
    "    # use a subfolder named by the current time to distinguish repeated runs\n",
    "    log_dir = os.path.join(\n",
    "        log_root,\n",
    "        datetime.datetime.now(\n",
    "            tz=pytz.timezone('Asia/Hong_Kong')).strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    \n",
    "    callbacks = fit_params.pop('callbacks', []).copy()\n",
    "    \n",
    "    if save_log_params is not None:\n",
    "        # add callback to save the training log for further analysis by tensorboard\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.TensorBoard(log_dir,\n",
    "                                           **save_log_params))\n",
    "\n",
    "    if save_model_params is not None:\n",
    "        # save the model as checkpoint files after each training epoch\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ModelCheckpoint(os.path.join(log_dir, '{epoch}.ckpt'),\n",
    "                                               **save_model_params))\n",
    "\n",
    "    if debug_params is not None:\n",
    "        # save information for debugger v2 in tensorboard\n",
    "        tf.debugging.experimental.enable_dump_debug_info(\n",
    "            log_dir, **debug_params)\n",
    "\n",
    "    # training + testing (validation)\n",
    "    model.fit(ds_b['train'],\n",
    "              validation_data=ds_b['test'],\n",
    "              callbacks=callbacks,\n",
    "              **fit_params)\n",
    "\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:52.989815Z",
     "start_time": "2021-02-26T11:11:25.693674Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_params = {'epochs': 6, 'callbacks': [tqdm.keras.TqdmCallback()], 'verbose': 0}\n",
    "log_root = 'private/demo/'\n",
    "save_log_params = {'update_freq': 100, 'histogram_freq': 1}\n",
    "save_model_params = {'save_weights_only': True, 'verbose': 1}\n",
    "debug_params = {'tensor_debug_mode': \"FULL_HEALTH\", 'circular_buffer_size': -1}\n",
    "\n",
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    model = compile_model(create_simple_model())\n",
    "    log_dir = train_model(model,\n",
    "                          fit_params = fit_params,\n",
    "                          log_root=log_root,\n",
    "                          save_log_params=save_log_params,\n",
    "                          save_model_params=save_model_params,\n",
    "                          debug_params=debug_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By providing the `save_model_params` to the callback [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_checkpoints_during_training), the model is saved at the end of each epoch to `log_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:53.172848Z",
     "start_time": "2021-02-26T11:11:52.992273Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model is useful because it often takes a long time to train a neural network. To reload the model from the latest checkpoint and continue to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:11:56.531764Z",
     "start_time": "2021-02-26T11:11:53.176594Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('Continue to train? [Y/n]').lower() != 'n':\n",
    "    # load the weights of the previously trained model\n",
    "    restored_model = compile_model(create_simple_model())\n",
    "    restored_model.load_weights(tf.train.latest_checkpoint(log_dir))    \n",
    "    # continue to train\n",
    "    train_model(restored_model, \n",
    "                log_root=log_root, \n",
    "                save_log_params=save_log_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By providing [`tf.keras.callbacks.TensorBoard`](https://www.tensorflow.org/tensorboard/get_started#using_tensorboard_with_keras_modelfit) as a callback function to the `fit` method earlier, the training logs can be analyzed using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:12:05.521678Z",
     "start_time": "2021-02-26T11:11:56.533404Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SCALARS` tab shows the curves of training and validation losses/accuracies after different batches/epoches. The curves often have jitters as the gradient descent is stochastic (random). To see the typical performance, a smoothing factor $\\theta\\in [0,1]$ can be applied on the left panel. The smoothed curve $\\bar{l}(t)$ of the original curve $l(t)$ is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{l}(t) = \\theta \\bar{l}(t-1) + (1-\\theta) l(t)\n",
    "\\end{align}\n",
    "\n",
    "which is called the moving average. Try changing the smoothing factor on the left panel to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  If the smoothing factor $\\theta$ is too large, would it cause bias when using empirical loss or performance to estimate the actual loss or performance? If so, is estimate overly optimistic or pessimistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:01:10.111999Z",
     "start_time": "2021-02-25T17:01:10.105064Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4591d3e2623444d89ade756d7a66fda2",
     "grade": true,
     "grade_id": "smoothing",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the input images in TensorBoard:\n",
    "- Run the following cell to write the images to the log directory.\n",
    "- Click the `refresh` button on the top of the previous TensorBoard panel.\n",
    "- Click the `IMAGE` tab to show the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:12:12.170105Z",
     "start_time": "2021-02-26T11:12:05.523304Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    with file_writer.as_default():\n",
    "        # Don't forget to reshape.\n",
    "        images = np.reshape([image for (image, label) in ds[\"train\"].take(25)],\n",
    "                            (-1, 28, 28, 1))\n",
    "        tf.summary.image(\"25 training data examples\",\n",
    "                         images,\n",
    "                         max_outputs=25,\n",
    "                         step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to presenting the results, TensorBoard is useful for debugging deep learning. In particular, learn\n",
    "- to check the model graph under the [`GRAPHS`](https://www.tensorflow.org/tensorboard/graphs) tab, \n",
    "- to debug using the [`DEBUGGER v2` tab](https://www.tensorflow.org/tensorboard/debugger_v2), and\n",
    "- to [publish your results](https://www.tensorflow.org/tensorboard/get_started#tensorboarddev_host_and_share_your_ml_experiment_results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can also show simultaneously the logs of different runs stored in different subfolders of the log directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:12:14.466414Z",
     "start_time": "2021-02-26T11:12:12.171615Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {log_root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select different runs on the left panel to compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that loading the log to TensorBoard may consume a lot of memory. You can list the TensorBoard notebook instances and kill those you do not need anymore by running `!kill {pid}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:12:14.496437Z",
     "start_time": "2021-02-26T11:12:14.468020Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "tb.notebook.list() # list all the running TensorBoard notebooks.\n",
    "\n",
    "pids_to_kill = [] # modify the list of pid to kill\n",
    "for pid in pids_to_kill: \n",
    "    !kill {pid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dc10e80c1e4512c7f292efcdbaa6c82",
     "grade": false,
     "grade_id": "check-dropout",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Exercise** Train the following network with [dropout](https://en.wikipedia.org/wiki/Dilution_(neural_networks)#Dropout). Try to tune the network for the best accuracy. Use `log_root='logs/dropout/'` to so your log will also be submitted along with your notebook. Put your training code inside the body of the conditional `if input...` for autograding to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:12:18.097130Z",
     "start_time": "2021-02-26T11:12:14.499032Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad01b057a2479f440822f7401b2a85b6",
     "grade": false,
     "grade_id": "dropout",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_dropout_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dropout(0.2),  # dropout\n",
    "        tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
    "    ], name=\"Dropout\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = compile_model(create_dropout_model())\n",
    "print(model.summary())\n",
    "log_root = 'logs/dropout/'\n",
    "\n",
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "131bc001b874963fb576d254bda5fa2c",
     "grade": false,
     "grade_id": "check-cnn",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Exercise** Explore the [convolutional neural network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network). Try to tune the network for the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:14:17.233240Z",
     "start_time": "2021-02-26T11:12:18.099294Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bbcdf67c7869da22da3e37de9e3fbbb",
     "grade": false,
     "grade_id": "cnn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32,\n",
    "                               3,\n",
    "                               activation='relu',\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ], name=\"CNN\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = compile_model(create_cnn_model())\n",
    "print(model.summary())\n",
    "log_root = 'logs/cnn/'\n",
    "\n",
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2b5414c836fcf9c8f130afc3ed00cff",
     "grade": false,
     "grade_id": "check-tb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Exercise** Launch TensorBoard to show the best performances of each of the two neural network architectures. Note that to clean up the log of the inferior results, you may need to kill the TensorBoard instance. It is easier to use the vscode interface or the terminal in the lab interface to remove folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:14:28.554625Z",
     "start_time": "2021-02-26T11:14:17.235338Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40fb5cf2c2f466d33eaf53b3ac1888e9",
     "grade": false,
     "grade_id": "tb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:05:07.830407Z",
     "start_time": "2021-02-26T11:05:07.827690Z"
    }
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with the result, you can deploy the model as a web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:15:50.562374Z",
     "start_time": "2021-02-26T11:15:50.415019Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to files in `mnist/model` that can be loaded by [`tensorflow.js`](https://www.tensorflow.org/js) on the page `mnist/index.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:17:47.747055Z",
     "start_time": "2021-02-26T11:17:45.160794Z"
    }
   },
   "outputs": [],
   "source": [
    "!tensorflowjs_converter --input_format keras 'model.h5' 'mnist/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the `mnist` folder, we first compress it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T20:34:24.350162Z",
     "start_time": "2021-02-26T20:34:24.082454Z"
    }
   },
   "outputs": [],
   "source": [
    "!zip -r mnist.zip mnist/* index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T19:41:21.843928Z",
     "start_time": "2021-02-26T19:41:21.839695Z"
    }
   },
   "source": [
    "Finally, you can download the zip file [here](./mnist.zip) and host the web application on a static web server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
