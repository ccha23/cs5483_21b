{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine vs Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a title=\"Freddycastillo9871, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Terminator_2.png\"><img width=\"512\" alt=\"Terminator 2\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/38/Terminator_2.png\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:30:04.314770Z",
     "start_time": "2021-02-20T04:30:03.024270Z"
    },
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "from weka.core import dataset\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier, Evaluation, SingleClassifierEnhancer\n",
    "from sklearn import ensemble, tree\n",
    "from weka.core.classes import Random\n",
    "from weka.core.classes import complete_classname\n",
    "from sklearn import ensemble\n",
    "from scipy.io import arff\n",
    "import urllib.request\n",
    "import io\n",
    "from joblib import Memory, Parallel, delayed, dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to build the best machine to classify the image segmentation dataset:\n",
    "- `segment-challenge.arff` for training, and\n",
    "- `segment-test.arff` for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weka provides a KnowledgeFlow interface to flow data through a learning algorithm. The following is a demo for training and testing a J48 decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:30:04.324635Z",
     "start_time": "2021-02-20T04:30:04.316978Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=3f33d95a-c5c2-4893-925b-acd10064acec&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f24fc432410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=3f33d95a-c5c2-4893-925b-acd10064acec&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\", height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the other interfaces, the evaluation results can be saved to files as demonstrated below. You may also load an existing template using a button in the toolbar on the top right-hand corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:30:04.340973Z",
     "start_time": "2021-02-20T04:30:04.327124Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=e60913e2-128b-4e40-962d-acd1006b8347&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f24fc47c110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=e60913e2-128b-4e40-962d-acd1006b8347&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\", height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike other interfaces (Explorer and Experimenter), KnowledgeFlow interface can train a classifier incrementally as more and more data are available. For more details, refer to the manual [here](https://www.cs.waikato.ac.nz/ml/weka/Witten_et_al_2016_appendix.pdf) and the [video tutorial](https://www.futurelearn.com/info/courses/more-data-mining-with-weka/0/steps/29106) by Witten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Explorer interface, we can flow data through multiple classification algorithms simultaneously as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:30:04.351462Z",
     "start_time": "2021-02-20T04:30:04.344310Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=0d3c11b4-7809-45c7-a208-acd1006ea2f1&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f24cbccfb10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=0d3c11b4-7809-45c7-a208-acd1006ea2f1&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\",  height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Follow the video above to add other classifiers: `IBk`, `ZeroR`, `OneR`, `PART`, and `JRIP`. Record their *fractional* accuracies in the dictionary `performance` as follows:\n",
    "\n",
    "```Python\n",
    "performance = {'J48': 0.961728,\n",
    "               'IBk': ___,\n",
    "               'ZeroR': ___,\n",
    "               'OneR': ___,\n",
    "               'PART': ___,\n",
    "               'JRIP': ___}\n",
    "```\n",
    "\n",
    "Use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.222644Z",
     "start_time": "2021-02-19T16:17:53.207100Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fdc0f9a1b555aa8fd103a684fe9a4cc",
     "grade": false,
     "grade_id": "kf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.234743Z",
     "start_time": "2021-02-19T16:17:53.225533Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9985b8f945cb9e70f0e13f27579bfedd",
     "grade": true,
     "grade_id": "test-kf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike training an individual classifier, ensemble methods\n",
    "- train an army of base classifiers and\n",
    "- combine their decisions into a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ensemble methods implemented in scikit-learn and Weka. To load the data for scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:10:36.570800Z",
     "start_time": "2021-02-20T03:10:35.869263Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_url(url):\n",
    "    ftpstream = urllib.request.urlopen(url)\n",
    "    df = pd.DataFrame(\n",
    "        arff.loadarff(io.StringIO(ftpstream.read().decode('utf-8')))[0])\n",
    "    return df.loc[:,\n",
    "                  lambda df: ~df.columns.isin(['class'])], df['class'].astype(\n",
    "                      str)\n",
    "\n",
    "\n",
    "weka_data_path = 'https://raw.githubusercontent.com/Waikato/weka-3.8/master/wekadocs/data/'\n",
    "X_train, Y_train = load_url(weka_data_path + 'segment-challenge.arff')\n",
    "X_test, Y_test = load_url(weka_data_path + 'segment-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data for `python-weka-wrapper` and start the Java virtual machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:10:38.808716Z",
     "start_time": "2021-02-20T03:10:37.850637Z"
    }
   },
   "outputs": [],
   "source": [
    "jvm.start()\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "trainset = loader.load_url(\n",
    "    weka_data_path +\n",
    "    'segment-challenge.arff')  # use load_file to load from file instead\n",
    "trainset.class_is_last()\n",
    "\n",
    "testset = loader.load_url(weka_data_path + 'segment-test.arff')\n",
    "testset.class_is_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple ensemble method is called Bagging (Bootstrap Aggregation), which trains different base classifiers by *applying a classification algorithm to different bootstrapped datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the following uses the [`sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to train 10 decision trees with maximum depth 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:54.933984Z",
     "start_time": "2021-02-19T16:17:54.862090Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "BAG = ensemble.BaggingClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0)\n",
    "\n",
    "BAG.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {BAG.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T05:31:30.833082Z",
     "start_time": "2021-02-19T05:31:30.365744Z"
    }
   },
   "source": [
    "The ensemble method can be parallelized for both training and classification, by setting the additional parameter `n_jobs` (number of jobs to run in parallel) to a number other than 1. Different jobs can be run in different CPU cores or threads. To see the effect, execute the following cell and answer `y` to the prompt or just press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:18:24.125245Z",
     "start_time": "2021-02-19T16:17:54.935346Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    for n_jobs in [1, 2, 4, 8]:\n",
    "        BAG.set_params(n_estimators=1000, verbose=1, n_jobs=n_jobs)\n",
    "        BAG.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a diminishing speedup as the number of jobs increases. This is because of both the memory and communication overheads required to parallelize the training. Concurrent access to the same memory can lead to unexpected behavior, and so it must be prevented by duplicating or locking the data, leading to the overheads. For more details, see the documentation [here](https://joblib.readthedocs.io/en/latest/parallel.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to see the effect of changing the depth and number of estimators as follows. The following are the lists of possible values to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:28:46.038026Z",
     "start_time": "2021-02-20T03:28:46.032306Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a function `bagging(n_estimators, max_depth)` that returns the accuracy of Bagging `n_estimators` decision trees of maximum depth `max_depth`. To avoid re-training/evaluating a classifier, we additionally cache the result using `joblib.Memory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:28:36.435643Z",
     "start_time": "2021-02-20T03:28:36.423671Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "import os\n",
    "# cache to private folder\n",
    "os.makedirs('private', exist_ok=True)\n",
    "memory = Memory(location=\"private\", verbose=0)\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def bagging(n_estimators, max_depth):\n",
    "    BAG = ensemble.BaggingClassifier(\n",
    "        base_estimator=tree.DecisionTreeClassifier(max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=0)\n",
    "    BAG.fit(X_train, Y_train)\n",
    "    return BAG.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, we can use `joblib` to run `bagging` in parallel for different values of `max_depth` and `n_estimators` stored in `max_depth_list` and `n_estimators_list` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:46:48.447940Z",
     "start_time": "2021-02-20T03:46:46.569391Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    BAG_results = Parallel(n_jobs=4,\n",
    "                           verbose=1)(delayed(bagging)(n_estimators, max_depth)\n",
    "                                      for max_depth in max_depth_list\n",
    "                                      for n_estimators in n_estimators_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `delayed(bagging)` is the lazy-version of `bagging` so that its invocation can be delayed until the task is assigned to a core/thread. The argument to `Parallel(n_jobs=4, verbose=1)` is a generator defined using python comprehension syntax. For more details, see the notebook [here](https://ccha23.github.io/CS1302ICP/Lecture6/More%20on%20Functions.html#generator) and the documentation [here](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the result nicely in a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:47:19.797213Z",
     "start_time": "2021-02-20T03:47:19.733016Z"
    }
   },
   "outputs": [],
   "source": [
    "def tabulate(results):\n",
    "    df = pd.DataFrame(\n",
    "        columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "        dtype=float)\n",
    "    df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    df.loc[:, lambda df: ~df.columns.isin(['n_estimators'])] = np.reshape(\n",
    "        results, (len(n_estimators_list), len(max_depth_list)), order='F')\n",
    "    return df\n",
    "\n",
    "\n",
    "BAG_df = tabulate(BAG_results)\n",
    "display.display(BAG_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above call `bagging` again for the same combinations of arguments, the cached results are returned without re-training the classifiers. You can clear the cache with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:29.935366Z",
     "start_time": "2021-02-19T16:19:29.815027Z"
    }
   },
   "outputs": [],
   "source": [
    "bagging.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:47:25.201080Z",
     "start_time": "2021-02-20T03:47:24.928674Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in df.columns[1:]:\n",
    "        plt.plot(df['n_estimators'], df[col], label=col, marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "\n",
    "\n",
    "plot(BAG_df)\n",
    "plt.title(r'Bagging decision trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to save the `DataFrame` to a particular file. This can be done using [`joblib.dump`](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:12:05.726892Z",
     "start_time": "2021-02-20T04:12:04.447744Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "if input('(over-)write file? [Y/n] ').lower() != 'n':\n",
    "    dump(BAG_df, 'BAG_df.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike caching, we can load the data anywhere beyond this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:51:38.336917Z",
     "start_time": "2021-02-20T03:51:38.321936Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "load('BAG_df.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T03:51:49.610975Z",
     "start_time": "2021-02-20T03:51:49.606006Z"
    }
   },
   "outputs": [],
   "source": [
    "os.remove('BAG_df.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** What happens to the accuracy as `n_estimators` and `max_depth` increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e88504de7fc48187b2c2820a3ed123f",
     "grade": true,
     "grade_id": "bag",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the ensemble method using `python-weka-wrapper` instead of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:37.900179Z",
     "start_time": "2021-02-19T16:19:37.527824Z"
    }
   },
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = ['-L', '5']\n",
    "BAG_weka = SingleClassifierEnhancer(classname=\"weka.classifiers.meta.Bagging\")\n",
    "BAG_weka.options = ['-I', '10', '-S', '1']\n",
    "BAG_weka.classifier = REPTree\n",
    "BAG_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(BAG_weka, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base classifiers for [`Bagging`](https://weka.sourceforge.io/doc.dev/weka/classifiers/meta/Bagging.html) are trained using [`REPTree`](https://weka.sourceforge.io/doc.dev/weka/classifiers/trees/REPTree.html), which is a fast decision tree induction algorithm that is neither C4.5 nor CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `BAG_weka_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. \n",
    "\n",
    "Note that we saved your result to a file `BAG_weka_df.gz` to avoid re-training. Otherwise, the server cannot auto-grade your submission as it aborts execution that takes excessive time or memory. Please also ensure that your code is properly indented so it is part of the body of the conditional in the solution cell:\n",
    "```Python\n",
    "...\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:02:08.581384Z",
     "start_time": "2021-02-20T04:01:56.608768Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e6020abf198b520c11245e1aa625886",
     "grade": false,
     "grade_id": "bag-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    BAG_weka_df = pd.DataFrame(\n",
    "        columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "        dtype=float)\n",
    "    BAG_weka_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    display.display(BAG_weka_df.round(4))\n",
    "\n",
    "    plot(BAG_weka_df)\n",
    "    plt.title(r'Bagging decision trees')\n",
    "    plt.show()\n",
    "\n",
    "    dump(BAG_weka_df, 'BAG_weka_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:06:53.427639Z",
     "start_time": "2021-02-20T04:06:53.406663Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "269b6d8e3dd13ab4a4ca3d3766b80492",
     "grade": true,
     "grade_id": "test-bag-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another ensemble method, called [random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), is similar to Bagging decision trees. However, to further diversify the base classifiers, it randomly selects or combines features before building each tree. The following trains a random forest of 10 decision trees with maximum depth 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:07:01.619922Z",
     "start_time": "2021-02-20T04:07:01.585187Z"
    }
   },
   "outputs": [],
   "source": [
    "RF = ensemble.RandomForestClassifier(max_depth=5, \n",
    "                                     n_estimators=10, \n",
    "                                     random_state=0)\n",
    "RF.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {RF.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Bagging, we can also parallelize the training and classification by setting the `n_jobs` parameter. In the above setting, however, the overhead out-weights the benefit, so it is better not to parallelize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `RF_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`, and with `random_state = 0`. You *must cache* your results properly to avoid re-training. Otherwise, the server cannot auto-grade your submission as it aborts execution that takes excessive time or memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:09:36.979399Z",
     "start_time": "2021-02-20T04:09:30.655752Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b96b6788e006a89e4459697876c0e7f",
     "grade": false,
     "grade_id": "rf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    RF_df = pd.DataFrame(\n",
    "        columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "        dtype=float)\n",
    "    RF_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(RF_df.round(4))\n",
    "\n",
    "    plot(RF_df)\n",
    "    plt.title(r'Random forest')\n",
    "    plt.show()\n",
    "\n",
    "    dump(RF_df, 'RF_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:09:47.927518Z",
     "start_time": "2021-02-20T04:09:47.904584Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abbec6c48885083db55e85ee80774fff",
     "grade": true,
     "grade_id": "test-rf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a random forest of 10 decision trees with maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:10:08.929066Z",
     "start_time": "2021-02-20T04:10:08.709668Z"
    }
   },
   "outputs": [],
   "source": [
    "RF_weka = Classifier(classname=\"weka.classifiers.trees.RandomForest\")\n",
    "RF_weka.options = ['-I', '10', '-depth', '5', '-S', '1']\n",
    "RF_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(RF_weka, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Repeat the previous exercise but with [Weka](https://weka.sourceforge.io/doc.dev/weka/classifiers/trees/RandomForest.html) instead. Use a random seed of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:13:38.660083Z",
     "start_time": "2021-02-20T04:13:36.360614Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d210af633762a380f4aa644559802d1",
     "grade": false,
     "grade_id": "rf-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    RF_weka_df = pd.DataFrame(\n",
    "        columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "        dtype=float)\n",
    "    RF_weka_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(RF_weka_df.round(4))\n",
    "\n",
    "    plot(RF_weka_df)\n",
    "    plt.title(r'Random forest')\n",
    "    plt.show()\n",
    "\n",
    "    dump(RF_weka_df, 'RF_weka_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:51:03.664932Z",
     "start_time": "2021-02-19T16:51:03.638743Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25b6fb9c7ec1cbb50444c23e5c3cbc7c",
     "grade": true,
     "grade_id": "test-rf-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** What are the best choices of `n_estimators` and `max_depth`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23969d8ed907bcab86313621f2e0383a",
     "grade": true,
     "grade_id": "rf-best",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), we can boost the performance by adding base classifiers one-by-one to improve the error made by previously trained classifiers. To train AdaBoost with 10 decision trees of maximum depth 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:13:52.410093Z",
     "start_time": "2021-02-20T04:13:52.271585Z"
    }
   },
   "outputs": [],
   "source": [
    "ADB = ensemble.AdaBoostClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0)\n",
    "ADB.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {ADB.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above is indeed the multi-class extension of AdaBoost called AdaBoost-SAMME. The original AdaBoost only work for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Unlike Bagging and random forest, the training of the base classifiers for AdaBoost cannot be parallelized. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5aa2d2cb9571df93fad853043012639",
     "grade": true,
     "grade_id": "adb-parallelize",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `ADB_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. Use `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:16:32.506225Z",
     "start_time": "2021-02-20T04:16:27.634565Z"
    },
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72fd7df14f0123094f73748fee6b9924",
     "grade": false,
     "grade_id": "adb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    ADB_df = pd.DataFrame(\n",
    "        columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "        dtype=float)\n",
    "    ADB_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(ADB_df.round(4))\n",
    "\n",
    "    plot(ADB_df)\n",
    "    plt.title(r'Adaboost decision trees')\n",
    "    plt.show()\n",
    "\n",
    "    dump(ADB_df, 'ADB_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:15:54.694934Z",
     "start_time": "2021-02-20T04:15:54.670071Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b12b581b292788fbd0a7515dc76c61",
     "grade": true,
     "grade_id": "test-adb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train [AdaBoost](https://weka.sourceforge.io/doc.dev/weka/classifiers/meta/AdaBoostM1.html) with 10 decision trees of maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:20:11.568900Z",
     "start_time": "2021-02-19T16:20:11.310406Z"
    }
   },
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = ['-L', '5']\n",
    "ADB_weka = SingleClassifierEnhancer(\n",
    "    classname=\"weka.classifiers.meta.AdaBoostM1\")\n",
    "ADB_weka.options = ['-I', '10', '-S', '1']\n",
    "ADB_weka.classifier = REPTree\n",
    "ADB_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(ADB_weka, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weka uses the multi-class extension called AdaboostM1, which is different from Adaboost-SAMME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Repeat the previous exercise but with Weka instead. Use a random seed of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:25:05.253048Z",
     "start_time": "2021-02-20T04:25:03.890007Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c5c24f75e0b0f588648331149a1eb3e",
     "grade": false,
     "grade_id": "adb-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    max_depth_list = [1, 2, 3, 5, 10]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "    ADB_weka_df = pd.DataFrame(\n",
    "        columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "        dtype=float)\n",
    "    ADB_weka_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(ADB_weka_df.round(4))\n",
    "\n",
    "    plot(ADB_weka_df)\n",
    "    plt.title(r'Adaboost decision trees')\n",
    "    plt.show()\n",
    "\n",
    "    dump(ADB_weka_df, 'ADB_weka_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:25:08.464683Z",
     "start_time": "2021-02-20T04:25:08.441486Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c90a31c6b4489d5799c10c4cdab9646",
     "grade": true,
     "grade_id": "test-adb-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Which ensemble method is better, Adaboost or random forest? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e676cdf3cbec4928fa23ca7795ce1e09",
     "grade": true,
     "grade_id": "rf-vs-adb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your own classifier to achieve the highest possible accuracies. You may:\n",
    "- choose different classification algorithms or ensemble methods such as Bagging, Stacking, Voting, XGBoost, etc.\n",
    "- tune the hyper-parameters manually or automatically using `GridSearchCV` in `scikit-learn` or `CVParameterSelection` in Weka.\n",
    "\n",
    "Post your model and results on [Canvas](https://canvas.cityu.edu.hk/courses/39808/discussion_topics/306324) to compete with others. If you want to include your code in this notebook, make sure you avoid excessive time or memory by putting your code in the body of the conditional `if input('execute? [Y/n] ').lower() != 'n':`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example using XGBoost with its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:28:05.629914Z",
     "start_time": "2021-02-20T04:28:04.259790Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    XGB = xgboost.XGBClassifier()\n",
    "    XGB.fit(X_train, Y_train)\n",
    "    print(f'Accuracy: {XGB.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:18:05.446432Z",
     "start_time": "2021-02-19T04:18:05.442861Z"
    }
   },
   "source": [
    "We can use `GridSearchCV` from `sklearn.model_selection` to tune the parameters for the best model. For instance, to tune `n_estimators` by searching for the best value from `n_estimators_list` that maximizes the cross-validated accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:34:01.772451Z",
     "start_time": "2021-02-19T16:33:48.075665Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "    param_grid = {\n",
    "        'n_estimators': n_estimators_list,\n",
    "        'max_depth': max_depth_list\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(ensemble.RandomForestClassifier(random_state=0),\n",
    "                               param_grid,\n",
    "                               verbose=1,\n",
    "                               n_jobs=4)\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    print(f'Accuracy: {grid_search.score(X_test, Y_test):.4g}')\n",
    "    print(f'Best parameters: {grid_search.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
