{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine vs Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a title=\"Freddycastillo9871, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Terminator_2.png\"><img width=\"512\" alt=\"Terminator 2\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/38/Terminator_2.png\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.173793Z",
     "start_time": "2021-02-19T16:17:53.022281Z"
    },
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "from weka.core import dataset\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier, Evaluation, SingleClassifierEnhancer\n",
    "from sklearn import ensemble, tree\n",
    "from weka.core.classes import Random\n",
    "from weka.core.classes import complete_classname\n",
    "from sklearn import ensemble\n",
    "from scipy.io import arff\n",
    "import urllib.request\n",
    "import io\n",
    "from joblib import Memory, Parallel, delayed, dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to build the best machine to classify the image segmentation dataset:\n",
    "- `segment-challenge.arff` for training, and\n",
    "- `segment-test.arff` for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weka provides a KnowledgeFlow interface to flow data through a learning algorithm. The following is a demo for training and testing a J48 decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.181007Z",
     "start_time": "2021-02-19T16:17:53.176402Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=3f33d95a-c5c2-4893-925b-acd10064acec&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\", height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the other interfaces, the evaluation results can be saved to files as demonstrated below. You may also load an existing template using a button in the toolbar on the top right-hand corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.193468Z",
     "start_time": "2021-02-19T16:17:53.183000Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=e60913e2-128b-4e40-962d-acd1006b8347&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\", height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the other interfaces, KnowledgeFlow interface can train a classifier incrementally as more and more data becomes available. For more details, refer to the manual [here](https://www.cs.waikato.ac.nz/ml/weka/Witten_et_al_2016_appendix.pdf) and the [video tutorial](https://www.futurelearn.com/info/courses/more-data-mining-with-weka/0/steps/29106) by Witten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Explorer interface, we can flow data through multiple classification algorithms simultaneously as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.205399Z",
     "start_time": "2021-02-19T16:17:53.195722Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=0d3c11b4-7809-45c7-a208-acd1006ea2f1&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\",  height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Follow the video above to add other classifiers: `IBk`, `ZeroR`, `OneR`, `PART`, and `JRIP`. Record their *fractional* accuracies in the dictionary `performance` as follows:\n",
    "\n",
    "```Python\n",
    "performance = {'J48': 0.961728,\n",
    "               'IBk': ___,\n",
    "               'ZeroR': ___,\n",
    "               'OneR': ___,\n",
    "               'PART': ___,\n",
    "               'JRIP': ___}\n",
    "```\n",
    "\n",
    "Use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.222644Z",
     "start_time": "2021-02-19T16:17:53.207100Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fdc0f9a1b555aa8fd103a684fe9a4cc",
     "grade": false,
     "grade_id": "kf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.234743Z",
     "start_time": "2021-02-19T16:17:53.225533Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9985b8f945cb9e70f0e13f27579bfedd",
     "grade": true,
     "grade_id": "test-kf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the ensemble methods implemented in scikit-learn and Weka to train armies of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following load the data for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:53.952370Z",
     "start_time": "2021-02-19T16:17:53.236145Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_url(url):\n",
    "    ftpstream = urllib.request.urlopen(url)\n",
    "    df = pd.DataFrame(arff.loadarff(io.StringIO(ftpstream.read().decode('utf-8')))[0])\n",
    "    return df.loc[:,lambda df: ~df.columns.isin(['class'])], df['class'].astype(str)\n",
    "\n",
    "weka_data_path = 'https://raw.githubusercontent.com/Waikato/weka-3.8/master/wekadocs/data/'\n",
    "X_train, Y_train = load_url(weka_data_path + 'segment-challenge.arff')\n",
    "X_test, Y_test = load_url(weka_data_path + 'segment-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loads the data for `python-weka-wrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:54.860216Z",
     "start_time": "2021-02-19T16:17:53.955032Z"
    }
   },
   "outputs": [],
   "source": [
    "jvm.start()\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "trainset = loader.load_url(\n",
    "    weka_data_path +\n",
    "    'segment-challenge.arff')  # use load_file to load from file instead\n",
    "trainset.class_is_last()\n",
    "\n",
    "testset = loader.load_url(weka_data_path + 'segment-test.arff')\n",
    "testset.class_is_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple ensemble method is called Bagging (Bootstrap Aggregation), which train different base classifiers by applying one classification algorithm to different bootstrapped datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the following uses the [`sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to train 10 decision trees with maximum depth 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:17:54.933984Z",
     "start_time": "2021-02-19T16:17:54.862090Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "BAG = ensemble.BaggingClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0)\n",
    "\n",
    "BAG.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {BAG.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T05:31:30.833082Z",
     "start_time": "2021-02-19T05:31:30.365744Z"
    }
   },
   "source": [
    "The ensemble method can be parallelized for both training and classification, by setting additional parameter `n_jobs` (number of jobs to run in parallel) to a number other than 1. To see the effect, execute the following cell and answer `y` to the prompt or just press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:18:24.125245Z",
     "start_time": "2021-02-19T16:17:54.935346Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    for n_jobs in [1, 2, 4, 8]:\n",
    "        BAG.set_params(n_estimators=1000, verbose=1, n_jobs = n_jobs)\n",
    "        BAG.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a diminishing speedup as the number of jobs increases. This is because of both the memory and communication overheads required to parallelize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to see the effect of changing the depth and number of estimators as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:18:24.130871Z",
     "start_time": "2021-02-19T16:18:24.127260Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-training a classifier, we can cache the result using `joblib.Memory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:18:24.147556Z",
     "start_time": "2021-02-19T16:18:24.132292Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Memory \n",
    "import os\n",
    "\n",
    "os.makedirs('private', exist_ok=True)\n",
    "memory = Memory(location=\"private\",verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def bagging(n_estimators, max_depth):\n",
    "    BAG = ensemble.BaggingClassifier(\n",
    "        base_estimator=tree.DecisionTreeClassifier(max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=0)\n",
    "    BAG.fit(X_train, Y_train)\n",
    "    return BAG.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run `bag` for different values of `max_depth` and `n_estimators` stored in `max_depth_list` and `n_estimators_list` respectively. The following parallelize the execution using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:25.954072Z",
     "start_time": "2021-02-19T16:18:24.148992Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    results = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(bagging)(n_estimators, max_depth)\n",
    "        for max_depth in max_depth_list for n_estimators in n_estimators_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the result nicely in a table and a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:29.812901Z",
     "start_time": "2021-02-19T16:19:25.956913Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    BAG_df = pd.DataFrame(\n",
    "    columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "    dtype=float)\n",
    "    BAG_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    BAG_df.loc[:, lambda df: ~df.columns.isin(['n_estimators'])] = np.reshape(\n",
    "        results, (len(n_estimators_list), len(max_depth_list)), order='F')\n",
    "    display.display(BAG_df)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in BAG_df.columns[1:]:\n",
    "        plt.plot(BAG_df['n_estimators'], BAG_df[col], label=col, marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Bagging decision trees')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above call `bagging` again for the same combinations of arguments, the cached results are returned without re-training the classifiers. You can clear the cache with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:29.935366Z",
     "start_time": "2021-02-19T16:19:29.815027Z"
    }
   },
   "outputs": [],
   "source": [
    "bagging.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to save the `DataFrame` to a particular file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:31.860634Z",
     "start_time": "2021-02-19T16:19:29.938505Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    dump(BAG_df, 'BAG_df.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike caching, we can load the data anywhere beyond this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:31.892970Z",
     "start_time": "2021-02-19T16:19:31.862368Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "load('BAG_df.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:22:03.322399Z",
     "start_time": "2021-02-19T16:22:03.318635Z"
    }
   },
   "outputs": [],
   "source": [
    "os.remove('BAG_df.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** What happens to the accuracy as `n_estimators` and `max_depth` increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e88504de7fc48187b2c2820a3ed123f",
     "grade": true,
     "grade_id": "bag",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `python-weka-wrapper` instead of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:37.900179Z",
     "start_time": "2021-02-19T16:19:37.527824Z"
    }
   },
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = ['-L', '5']\n",
    "BAG_weka = SingleClassifierEnhancer(classname=\"weka.classifiers.meta.Bagging\")\n",
    "BAG_weka.options=['-I', '10',\n",
    "             '-S', '1']\n",
    "BAG_weka.classifier = REPTree\n",
    "BAG_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(BAG_weka, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base classifiers for `Bagging` are trained using `REPTree`, which is a fast decision tree induction algorithm that is neither C4.5 nor CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `BAG_weka_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. \n",
    "\n",
    "Note that we saved your result to a file `BAG_weka_df.gz` to avoid re-training. Otherwise, the server cannot auto-grade your submission as it aborts execution that takes excessive time or memory. Please also ensure that your code is under the body of the conditional `if input('execute? [Y/n] ').lower() != 'n':`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:46:14.089386Z",
     "start_time": "2021-02-19T16:46:12.024416Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b506f982d3bca32cd2140cd535eb61fc",
     "grade": false,
     "grade_id": "bag-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    BAG_weka_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    BAG_weka_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    display.display(BAG_weka_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in BAG_weka_df.columns[1:]:\n",
    "        plt.plot(BAG_weka_df['n_estimators'],\n",
    "                 BAG_weka_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Bagging decision trees')\n",
    "    plt.show()\n",
    "    \n",
    "    dump(BAG_weka_df, 'BAG_weka_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:50:15.126981Z",
     "start_time": "2021-02-19T16:50:15.105142Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "269b6d8e3dd13ab4a4ca3d3766b80492",
     "grade": true,
     "grade_id": "test-bag-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another ensemble method, called random forest, is similar to Bagging decision trees. However, to further diversify the base classifiers, it randomly selects or combines features before building each tree. The following trains a random forest of 10 decision trees with maximum depth 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:39.969587Z",
     "start_time": "2021-02-19T16:19:39.927960Z"
    }
   },
   "outputs": [],
   "source": [
    "RF = ensemble.RandomForestClassifier(max_depth=5, \n",
    "                                     n_estimators=10, \n",
    "                                     random_state=0)\n",
    "RF.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {RF.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Bagging, we can also parallelize the training and classification by setting the `n_jobs` parameter. In the above setting, however, the overhead out-weights the benefit, so it is better not to parallelize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `RF_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`, and with `random_state = 0`. You *must cache* your results properly to avoid re-training. Otherwise, the server cannot auto-grade your submission as it aborts execution that takes excessive time or memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:48:43.330571Z",
     "start_time": "2021-02-19T16:48:41.491140Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc701eac7e21e40b740e74fcf18c839d",
     "grade": false,
     "grade_id": "rf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    RF_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    RF_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(RF_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in RF_df.columns[1:]:\n",
    "        plt.plot(RF_df['n_estimators'],\n",
    "                 RF_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Random forest')\n",
    "    plt.show()\n",
    "\n",
    "    dump(RF_df, 'RF_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:50:21.608209Z",
     "start_time": "2021-02-19T16:50:21.576876Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abbec6c48885083db55e85ee80774fff",
     "grade": true,
     "grade_id": "test-rf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a random forest of 10 decision trees with maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:19:52.912636Z",
     "start_time": "2021-02-19T16:19:52.792327Z"
    }
   },
   "outputs": [],
   "source": [
    "RF_weka = Classifier(classname=\"weka.classifiers.trees.RandomForest\")\n",
    "RF_weka.options = ['-I', '10',\n",
    "              '-depth', '5',\n",
    "              '-S', '1']\n",
    "RF_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(RF_weka, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Repeat the previous exercise but with Weka instead. Use a random seed of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:51:00.076859Z",
     "start_time": "2021-02-19T16:50:57.751923Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adeadf743e5327257eb919d3e4c3282a",
     "grade": false,
     "grade_id": "rf-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    RF_weka_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    RF_weka_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(RF_weka_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in RF_weka_df.columns[1:]:\n",
    "        plt.plot(RF_weka_df['n_estimators'],\n",
    "                 RF_weka_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Random forest')\n",
    "    plt.show()\n",
    "\n",
    "    dump(RF_weka_df, 'RF_weka_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:51:03.664932Z",
     "start_time": "2021-02-19T16:51:03.638743Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25b6fb9c7ec1cbb50444c23e5c3cbc7c",
     "grade": true,
     "grade_id": "test-rf-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** What are the best choices of `n_estimators` and `max_depth`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23969d8ed907bcab86313621f2e0383a",
     "grade": true,
     "grade_id": "rf-best",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AdaBoost, we can boost the performance by adding base classifiers one-by-one to improve the error made by previously trained classifiers. To train AdaBoost with 10 decision trees of maximum depth 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:20:07.547788Z",
     "start_time": "2021-02-19T16:20:07.419755Z"
    }
   },
   "outputs": [],
   "source": [
    "ADB = ensemble.AdaBoostClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0)\n",
    "ADB.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {ADB.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Unlike Bagging and random forest, the training of the base classifiers for AdaBoost cannot be parallelized. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5aa2d2cb9571df93fad853043012639",
     "grade": true,
     "grade_id": "adb-parallelize",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `ADB_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. Use `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:52:57.908956Z",
     "start_time": "2021-02-19T16:52:56.467276Z"
    },
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "982ecb15fe49b640bac4472787f76783",
     "grade": false,
     "grade_id": "adb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    ADB_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    ADB_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(ADB_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in ADB_df.columns[1:]:\n",
    "        plt.plot(ADB_df['n_estimators'],\n",
    "                 ADB_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Adaboost decision trees')\n",
    "    plt.show()\n",
    "    \n",
    "    dump(ADB_df, 'ADB_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:53:03.195663Z",
     "start_time": "2021-02-19T16:53:03.166863Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b12b581b292788fbd0a7515dc76c61",
     "grade": true,
     "grade_id": "test-adb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train AdaBoost with 10 decision trees of maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:20:11.568900Z",
     "start_time": "2021-02-19T16:20:11.310406Z"
    }
   },
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = ['-L', '5']\n",
    "ADB_weka = SingleClassifierEnhancer(classname=\"weka.classifiers.meta.AdaBoostM1\")\n",
    "ADB_weka.options=['-I', '10',\n",
    "             '-S', '1']\n",
    "ADB_weka.classifier = REPTree\n",
    "ADB_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(ADB_weka, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Repeat the previous exercise but with Weka instead. Use a random seed of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:53:15.883825Z",
     "start_time": "2021-02-19T16:53:14.724230Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf21250bf6f94eacfbcd79ab521bbae7",
     "grade": false,
     "grade_id": "adb-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    max_depth_list = [1, 2, 3, 5, 10]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "    ADB_weka_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    ADB_weka_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(ADB_weka_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in ADB_weka_df.columns[1:]:\n",
    "        plt.plot(ADB_weka_df['n_estimators'],\n",
    "                 ADB_weka_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Adaboost decision trees')\n",
    "    plt.show()\n",
    "    \n",
    "    dump(ADB_weka_df, 'ADB_weka_df.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:53:19.723483Z",
     "start_time": "2021-02-19T16:53:19.695423Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c90a31c6b4489d5799c10c4cdab9646",
     "grade": true,
     "grade_id": "test-adb-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Which ensemble method is better, Adaboost or random forest? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e676cdf3cbec4928fa23ca7795ce1e09",
     "grade": true,
     "grade_id": "rf-vs-adb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your own classifier to achieve the highest possible accuracies. You may:\n",
    "- choose different classification algorithms or ensemble methods such as Bagging, Stacking, Voting, XGBoost, etc.\n",
    "- tune the hyper-parameters manually or automatically using `GridSearchCV` in `scikit-learn` or `CVParameterSelection` in Weka.\n",
    "\n",
    "Post your model and results on [Canvas](https://canvas.cityu.edu.hk/courses/39808/discussion_topics/306324) to compete with others. If you want to include your code in this notebook, make sure you avoid excessive time or memory by putting your code in the body of the conditional `if input('execute? [Y/n] ').lower() != 'n':`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example using XGBoost with its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:35:58.097952Z",
     "start_time": "2021-02-19T16:35:18.702773Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    XGB = xgboost.XGBClassifier()\n",
    "    XGB.fit(X_train, Y_train)\n",
    "    print(f'Accuracy: {XGB.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:18:05.446432Z",
     "start_time": "2021-02-19T04:18:05.442861Z"
    }
   },
   "source": [
    "We can use `GridSearchCV` from `sklearn.model_selection` to tune the parameters for the best model. For instance, to tune `n_estimators` by searching for the best value from `n_estimators_list` that maximizes the cross-validated accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T16:34:01.772451Z",
     "start_time": "2021-02-19T16:33:48.075665Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "    param_grid={'n_estimators': n_estimators_list, 'max_depth': max_depth_list}\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        ensemble.RandomForestClassifier(random_state=0), \n",
    "        param_grid, verbose=1, n_jobs=4)\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    print(f'Accuracy: {grid_search.score(X_test, Y_test):.4g}')\n",
    "    print(f'Best parameters: {grid_search.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
