{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Induction with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T04:46:41.955597Z",
     "start_time": "2021-02-20T04:46:40.812410Z"
    },
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "851cc20d47e07396bc98793d282e9860",
     "grade": false,
     "grade_id": "init",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, tree\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the [*iris dataset*](https://en.wikipedia.org/wiki/Iris_flower_data_set) from [`sklearn.datasets` package](https://scikit-learn.org/stable/datasets/index.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T09:26:34.571982Z",
     "start_time": "2021-01-24T09:26:34.563579Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the classification task is to train a model that can classify the spieces (*target*) automatically based on the lengths and widths of the petals and sepals (*input features*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a decision tree, we simply create a tree using `DecisionTreeClassifier` from `sklearn.tree` and apply its method `fit` on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T09:53:09.983034Z",
     "start_time": "2021-01-24T09:53:09.978881Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_gini = tree.DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T12:03:34.659657Z",
     "start_time": "2021-01-22T12:03:34.656984Z"
    }
   },
   "source": [
    "To display the decision tree, we can use the function `plot_tree` from `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T09:57:57.179896Z",
     "start_time": "2021-01-24T09:57:56.575574Z"
    }
   },
   "outputs": [],
   "source": [
    "# to make the tree look better\n",
    "options = {'feature_names': iris.feature_names, \n",
    "           'class_names': iris.target_names,\n",
    "           'filled': True,\n",
    "           'node_ids': True,\n",
    "           'rounded': True,\n",
    "           'fontsize': 6}\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "tree.plot_tree(clf_gini, **options)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each node:\n",
    "- `___ <= ___` is the splitting criterion for internal nodes, satisfied only by samples going left.\n",
    "- `gini = ...` shows the impurity index. By default, the algorithm uses Gini impurity index to find the best binary split. Observe that the index decreases down the tree towards the leafs.\n",
    "- `value = [_, _, _]` shows the number of examples for each of the three classes, and `class = ...` indicates a majority class, which may be used as the decision for a leaf node. The majority classes are also color coded. Observe that the color gets lighter towards the root, as the class distribution is more impure. \n",
    "\n",
    "In particular, check that iris setosa is distinguished immediately after checking the petal width/length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the information of the decision is stored in the `tree_` attribute of the classifer. For more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T10:15:30.624458Z",
     "start_time": "2021-01-24T10:15:30.616057Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "help(clf_gini.tree_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Assign to `clf_entropy` the decision tree classifier created using *entropy* as the impurity measure. You can do so with the keyword argument `criterion='entropy'` in `DecisionTreeClassifier`. Furthermore, Use `random_state=0` and fit the classifier on the entire iris dataset. Check whether the resulting decision tree same as the one created using the Gini impurity index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T10:25:21.769095Z",
     "start_time": "2021-01-24T10:25:21.173622Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b5af6f05d8ce6f84d7a2178a5ed5a53",
     "grade": false,
     "grade_id": "tree-entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "tree.plot_tree(clf_entropy, **options)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b33243c1128464d72060ac89b3646dd1",
     "grade": true,
     "grade_id": "same-tree-as-gini",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that, although one can specify whether to use Gini impurity or entropy, `sklearn` implements neither C4.5 nor CART algorithms. In particular, it supports only binary splits on numeric input attributes, unlike C4.5 which supports multi-way splits using information gain ratio.  \n",
    "(See some [workarounds][categorical].)\n",
    "\n",
    "[categorical]: https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Splitting Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the decision tree is generated, we will implements the computation of the splitting criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data analysis using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea of qualities of the features, create a [`pandas`](https://pandas.pydata.org/docs/user_guide/index.html) [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame) \n",
    "to operate on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T10:53:58.672726Z",
     "start_time": "2021-01-24T10:53:58.653480Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9gQINfrjsb4M",
    "outputId": "77b77a38-2712-4c93-c503-219e74f354fd",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# write the input features first\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# append the target values to the last column\n",
    "iris_df['target'] = iris.target\n",
    "iris_df.target = iris_df.target.astype('category')\n",
    "iris_df.target.cat.categories = iris.target_names\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display some statistics of the input features for different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:58:31.032422Z",
     "start_time": "2021-01-24T14:58:30.591901Z"
    }
   },
   "outputs": [],
   "source": [
    "iris_df.groupby('target').boxplot(rot=90, layout=(1,3))\n",
    "iris_df.groupby('target').agg(['mean','std']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Identify good feature(s) based on the above statistics. Does you choice agree with the decision tree generated by `DecisionTreeClassifier`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23015178e587e6f2ad5fd85735da2221",
     "grade": true,
     "grade_id": "good-features",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a distribution $\\boldsymbol{p}=(p_1,p_2,\\dots)$ where $p_k\\in [0,1]$ and $\\sum_k p_k =1$, the Gini impurity index is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{Gini}(\\boldsymbol{p}) = \\operatorname{Gini}(p_1,p_2,\\dots) &:= \\sum_k p_k(1-p_k)\\\\\n",
    "&= 1- \\sum_k p_k^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent a distribution simply as a `numpy` array. To return the empirical class distributions of the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T15:07:28.662385Z",
     "start_time": "2021-01-24T15:07:28.657990Z"
    }
   },
   "outputs": [],
   "source": [
    "def dist(values):\n",
    "    '''Returns the empirical distribution of the given 1D array of values as a \n",
    "    1D array of probabilites.'''\n",
    "    counts = np.unique(values, return_counts=True)[-1]\n",
    "    return counts / counts.sum()\n",
    "\n",
    "\n",
    "print(f\"Distribution of target: {dist(iris.target).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini impurity index can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T15:08:12.295668Z",
     "start_time": "2021-01-24T15:08:12.288803Z"
    }
   },
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "    '''Returns the Gini impurity of distribution p.'''\n",
    "    return 1 - (p**2).sum()\n",
    "\n",
    "print(f\"Gini impurity of target: {gini(dist(iris.target)):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the following function to compute the entropy of a distribution:\n",
    "\n",
    "\\begin{align}\n",
    "h(\\boldsymbol{p}) = h(p_1,p_2,\\dots) &= \\sum_k - p_k \\log_2 p_k\\\\\n",
    "&= \\sum_{k:p_k>0} - p_k \\log_2 p_k.\n",
    "\\end{align}\n",
    "\n",
    "You may use the function `log2` from `numpy` to calculate the logarithm base 2. Note that logarithm of $0$ is undefined, we use the last expression of the entropy to avoid taking the limit $\\lim_{p\\to 0} p\\log p=0$.\n",
    "\n",
    "You solution should look like:\n",
    "```Python\n",
    "def entropy(p):\n",
    "    ...\n",
    "    return (p * ___ * ___).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T13:49:49.051166Z",
     "start_time": "2021-01-24T13:49:49.047003Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f18b27cbcc409dcd4a353adfe721bbc2",
     "grade": false,
     "grade_id": "entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    '''Returns the entropy of distribution p.'''\n",
    "    p = np.array(p)\n",
    "    p = p[(p > 0) & (p < 1)]  # 0 log 0 = 1 log 1 = 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:08:06.915073Z",
     "start_time": "2021-01-24T14:08:06.910946Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6413a8489e09d23de03ae05263557271",
     "grade": true,
     "grade_id": "test-entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(entropy([1/2, 1/2]), 1)\n",
    "assert np.isclose(entropy([1, 0]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute drop in impurity and best split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute the drop in impurity for given splitting criterion:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\operatorname{Gini}_{X\\leq s}(Y) = \\operatorname{Gini}(P_Y) - \\left[\\Pr\\{X\\leq s\\} \\operatorname{Gini}(P_{Y|X\\leq s}) + \\Pr\\{X> s\\}\\operatorname{Gini}(P_{Y|X> s})\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T11:54:04.419724Z",
     "start_time": "2021-01-24T11:54:04.407561Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_in_gini(X, Y, split_pt):\n",
    "    '''Returns the drop in Gini impurity of Y for the split X <= split_pt.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 1D array the values of a feature for different instances\n",
    "    Y: 1D array of the corresponding target values\n",
    "    split_pt: the value of the split point\n",
    "    '''\n",
    "    S = X <= split_pt\n",
    "    q = S.mean()\n",
    "    return gini(dist(Y)) - q * gini(dist(Y[S])) - (1 - q) * gini(dist(Y[~S]))\n",
    "\n",
    "X, Y = iris_df['petal width (cm)'], iris_df.target\n",
    "print(f\"Drop in Gini: {drop_in_gini(X, Y, 0.8):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the best split point for a feature, we check every consecutive mid-points of the possible feature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T11:56:35.275963Z",
     "start_time": "2021-01-24T11:56:35.243196Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_best_split_pt(X, Y, gain_function):\n",
    "    '''Return the best split point s and the maximum gain evaluated using \n",
    "    gain_function for the split X <= s and target Y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 1D array the values of a feature for different instances\n",
    "    Y: 1D array of the corresponding target values\n",
    "    gain_function: a function such as drop_in_gini for evaluating a split\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple (s, g) where s is the best split point and g is the maximum gain.\n",
    "    \n",
    "    See also\n",
    "    --------\n",
    "    drop_in_gini\n",
    "    '''\n",
    "    values = np.sort(np.unique(X))\n",
    "    split_pts = (values[1:] + values[:-1]) / 2\n",
    "    gain = np.array([gain_function(X, Y, s) for s in split_pts])\n",
    "    max_index = np.argmax(gain)\n",
    "    return split_pts[max_index], gain[max_index]\n",
    "\n",
    "\n",
    "print('''Best split point: {0:.4g}\n",
    "Maximum gain: {1:.4g}'''.format(*find_best_split_pt(X, Y, drop_in_gini)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T12:15:40.237585Z",
     "start_time": "2021-01-24T12:15:40.231097Z"
    }
   },
   "source": [
    "The following ranks the features according to the gains of their best binary splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:55:41.967757Z",
     "start_time": "2021-01-24T14:55:41.811034Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_by_gini = pd.DataFrame({\n",
    "    'feature': feature,\n",
    "    **(lambda s, g: {\n",
    "        'split point': s,\n",
    "        'gain': g\n",
    "    })(*find_best_split_pt(iris_df[feature], iris_df.target, drop_in_gini))\n",
    "} for feature in iris.feature_names).sort_values(by='gain', ascending=False)\n",
    "rank_by_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the following function to calculate the *information gain* for a binary split $X\\leq s$ and target $Y$:\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{Gain}_{X\\leq s}(Y) := h(P_Y) - \\left[\\Pr(X\\leq s) h(P_{Y|X\\leq s}) + \\Pr(X> s) h(P_{Y|X> s})\\right].\n",
    "\\end{align}\n",
    "\n",
    "You may use `dist` and `entropy` defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:15:35.911367Z",
     "start_time": "2021-01-24T14:15:35.899677Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b4d69805318711d8c61770cafe6922",
     "grade": false,
     "grade_id": "info-gain",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def info_gain(X, Y, split_pt):\n",
    "    '''Returns the information Gain of Y for the split X <= split_pt.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 1D array the values of a feature for different instances\n",
    "    Y: 1D array of the corresponding target values\n",
    "    split_pt: the value of the split point\n",
    "    '''\n",
    "    S = X <= split_pt\n",
    "    q = S.mean()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(f\"Information gain: {info_gain(X, Y, 0.8):.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:55:08.980787Z",
     "start_time": "2021-01-24T14:55:08.847163Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10a933cf9c02c5de17b080f633d449f3",
     "grade": true,
     "grade_id": "test-info-gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "rank_by_entropy = pd.DataFrame({\n",
    "    'feature':\n",
    "    feature,\n",
    "    **(lambda s, g: {\n",
    "        'split point': s,\n",
    "        'gain': g\n",
    "    })(*find_best_split_pt(iris_df[feature], iris_df.target, info_gain))\n",
    "} for feature in iris.feature_names).sort_values(by='gain', ascending=False)\n",
    "rank_by_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the following function to calculate the *information gain ratio* for a binary split $X\\leq s$ and target $Y$:\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{GainRatio}_{X\\leq s}(Y) &:= \\frac{\\operatorname{Gain}_{X\\leq s}(Y)}{\\operatorname{SplitInfo}(X\\leq s)} \\qquad\\text{where}\\\\\n",
    "\\operatorname{SplitInfo}(X\\leq s) &:= h(\\Pr(X\\leq s),\\Pr(X> s)).\n",
    "\\end{align}\n",
    "\n",
    "You may use `entropy` and `info_gain` defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:53:22.588230Z",
     "start_time": "2021-01-24T14:53:22.581141Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d741235b484b516c1bf725fb66ef845",
     "grade": false,
     "grade_id": "info-gain-ratio",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def info_gain_ratio(X, Y, split_pt):\n",
    "    S = X <= split_pt\n",
    "    q = S.mean()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T15:00:35.323965Z",
     "start_time": "2021-01-24T15:00:35.182551Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc8476135900c3ab349e9d4643fd3640",
     "grade": true,
     "grade_id": "test-info-gain-ratio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "rank_by_info_gain_ratio = pd.DataFrame({\n",
    "    'feature':\n",
    "    feature,\n",
    "    **(lambda s, g: {\n",
    "        'split point': s,\n",
    "        'gain': g\n",
    "    })(*find_best_split_pt(iris_df[feature], iris_df.target, info_gain_ratio))\n",
    "} for feature in iris.feature_names).sort_values(by='gain', ascending=False)\n",
    "rank_by_info_gain_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Does the information gain ratio give a different ranking of the features? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain ratio gives the same ranking as information gain in this case. This is because the split is restricted to be binary and so the normalization by split information has less effect on the ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "beige"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
