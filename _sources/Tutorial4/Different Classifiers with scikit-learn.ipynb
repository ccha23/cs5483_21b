{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Classifiers with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T13:19:08.198285Z",
     "start_time": "2021-02-01T13:19:07.119715Z"
    },
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90130ec12203b8a081ca9f48159da8ca",
     "grade": false,
     "grade_id": "init",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, tree, preprocessing, neighbors\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats, display\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "set_matplotlib_formats('svg')\n",
    "from wittgenstein import RIPPER\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we consider the binary classification problem on the [breast cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:34.175973Z",
     "start_time": "2021-02-01T12:50:34.133315Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# load the dataset from sklearn\n",
    "dataset = datasets.load_breast_cancer()\n",
    "\n",
    "# create a DataFrame to help further analysis\n",
    "df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "df['target'] = dataset.target\n",
    "df.target = df.target.astype('category')\n",
    "df.target.cat.categories = dataset.target_names\n",
    "df  # display an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a classifier to diagnose whether a breast mass is malignant or benign. The target class distribution is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:34.331940Z",
     "start_time": "2021-02-01T12:50:34.177731Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.target.value_counts())\n",
    "df.target.value_counts().plot(kind='bar', title='counts of different classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input features are characteristics of cell images obtained by [fine needle analysis (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T13:19:08.206035Z",
     "start_time": "2021-02-01T13:19:08.200140Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://www.researchgate.net/figure/FNA-biopsy-samples-of-benign-left-and-malignant-center-and-right-breast-tumor-cells_fig1_261959799\"><img src=\"https://www.researchgate.net/profile/Carsten_Eickhoff/publication/261959799/figure/fig1/AS:296624837414914@1447732280878/FNA-biopsy-samples-of-benign-left-and-malignant-center-and-right-breast-tumor-cells.png\" alt=\"FNA biopsy samples of benign (left) and malignant (center and right) breast tumor cells.\"/></a>\n",
       "<p>FNA biopsy samples of benign (left) and malignant (center and right) breast tumor cells.</p>\n",
       "<p>Eickhoff, Carsten. (2014). Crowd-powered experts: helping surgeons interpret breast cancer images. ACM International Conference Proceeding Series. 53-56. 10.1145/2594776.2594788.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"https://www.researchgate.net/figure/FNA-biopsy-samples-of-benign-left-and-malignant-center-and-right-breast-tumor-cells_fig1_261959799\"><img src=\"https://www.researchgate.net/profile/Carsten_Eickhoff/publication/261959799/figure/fig1/AS:296624837414914@1447732280878/FNA-biopsy-samples-of-benign-left-and-malignant-center-and-right-breast-tumor-cells.png\" alt=\"FNA biopsy samples of benign (left) and malignant (center and right) breast tumor cells.\"/></a>\n",
    "<p>FNA biopsy samples of benign (left) and malignant (center and right) breast tumor cells.</p>\n",
    "<p>Eickhoff, Carsten. (2014). Crowd-powered experts: helping surgeons interpret breast cancer images. ACM International Conference Proceeding Series. 53-56. 10.1145/2594776.2594788.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function displays the statistics of the features grouped by the class values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:35.357961Z",
     "start_time": "2021-02-01T12:50:34.341674Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_feature_statistics(df):\n",
    "    df.groupby('target').boxplot(rot=90,\n",
    "                                 layout=(1, 3),\n",
    "                                 figsize=(12, 5),\n",
    "                                 fontsize=7)\n",
    "\n",
    "\n",
    "show_feature_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, it can be observed that the attributes `mean area` and `worst area` have much larger ranges than other features have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Is it true that a feature with the larger range is a better feature? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c4db5e8ba428be653c58f92819cf9c0",
     "grade": true,
     "grade_id": "range",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize a numeric feature to the unit interval as follows:\n",
    "\n",
    "\\begin{align}\n",
    "X_i\\in [X_i^{\\min}, X_i^{\\max}] \\mapsto \\frac{X_i - X_i^{\\min}}{X_i^{\\max} - X_i^{\\min}} \\in [0,1].\n",
    "\\end{align}\n",
    "\n",
    "To perform the normalization on i.i.d. samples $x_{ij}$ indexed by $j$, we use the mapping\n",
    "\n",
    "\\begin{align}\n",
    "x_{ij} \\mapsto \\frac{x_{ij} - \\min_j x_j}{\\max_j x_j - \\min_j x_j}.\n",
    "\\end{align}\n",
    "\n",
    "An implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:35.363268Z",
     "start_time": "2021-02-01T12:50:35.359442Z"
    }
   },
   "outputs": [],
   "source": [
    "def minmax_normalize(df, suffix=' (min-max normalized)'):\n",
    "    ''' Returns a DataFrame with numerical attributes of the input DataFrame \n",
    "    min-max normalized.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df (DataFrame): Input to be min-max normalized. May contain both numeric \n",
    "        and categorical attributes. \n",
    "    suffix (string): Suffix to append to the names of normalized attributes.\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    A new DataFrame which retains the categorical attributes but with the \n",
    "    numeric attributes replaced by their min-max normalization. \n",
    "    The normalized features are renamed with the suffix appended to the end of \n",
    "    their original names.\n",
    "    '''\n",
    "    df_minmax_normalized = df.copy()\n",
    "    min_values = df.min()  # Categorical feature target skipped automatically\n",
    "\n",
    "    # min-max normalize\n",
    "    df_minmax_normalized[min_values.index] = ((df[min_values.index] - min_values) /\n",
    "                            (df.max() - min_values)).copy()\n",
    "    \n",
    "    # rename normalized features\n",
    "    df_minmax_normalized.rename(\n",
    "        columns={c: c + suffix\n",
    "                 for c in min_values.index},\n",
    "        inplace=True)\n",
    "\n",
    "    return df_minmax_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to rename the normalized features to differentiate them from the original features. The following plots the statistics of the normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:36.569473Z",
     "start_time": "2021-02-01T12:50:35.364574Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax_normalized = minmax_normalize(df)\n",
    "assert df_minmax_normalized.target.to_numpy().base is df.target.to_numpy().base\n",
    "show_feature_statistics(df_minmax_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalization, we can see how instances of different classes differ in different input features other than `mean area` and `worst area`. In particular, both `mean-concavity` and `worst-concavity` are substantially higher for malignant examples than for benign examples. Such details are hard to see in the plots before normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For features with unbounded support and a distribution close to the normal distribution, one may use the $z$-score/standard normalization instead of min-max normalization:\n",
    "\n",
    "\\begin{align}\n",
    "X_i \\mapsto \\frac{X_i - \\mu_{X_i}}{\\sigma_{X_i}},\n",
    "\\end{align}\n",
    "\n",
    "where $\\mu_{X_i}$ and $\\sigma_{X_i}$ are the expectation and standard deviation of the feature $X_i$ respectively. The i.i.d.\\ samples can be transformed using sample mean and standard deviation instead. It is okay to use a biased estimate for standard deviation and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the distribution of each feature, we can use the function [`displot`](https://seaborn.pydata.org/generated/seaborn.displot.html) provided by the package [`seaborn`](https://seaborn.pydata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:36.787536Z",
     "start_time": "2021-02-01T12:50:36.571875Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "@interact(feature=dataset.feature_names, kernel_density_estimation=True, group_by_class=False)\n",
    "def plot_distribution(feature, kernel_density_estimation, group_by_class):\n",
    "    sns.displot(data=df, x=feature, col='target' if group_by_class else None, kde=kernel_density_estimation, height=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the above widgets to check if the distributions of a feature look like the bell-shaped normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the function `standard_normalize` as follows:\n",
    "\n",
    "- Assign to `df_standard_normalized` a new data `DataFrame` same as `df` but with all its numeric attributes standard normalized. \n",
    "- You may use the methods `mean` and `std`.\n",
    "- Rename the normalized features by appending `suffix` to their names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.040118Z",
     "start_time": "2021-02-01T12:50:36.789409Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf51b22ef9e65c7e7446cf0033791cda",
     "grade": false,
     "grade_id": "normalize-attributes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def standard_normalize(df, suffix=' (standard normalized)'):\n",
    "    ''' Returns a DataFrame with numerical attributes of the input DataFrame \n",
    "    standard normalized.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df (DataFrame): Input to be standard normalized. May contain both numeric \n",
    "        and categorical attributes. \n",
    "    suffix (string): Suffix to append to the names of normalized attributes.\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    A new DataFrame which retains the categorical attributes but with the \n",
    "    numeric attributes replaced by their standard normalization. \n",
    "    The normalized features are renamed with the suffix appended to the end of \n",
    "    their original names.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df_standard_normalized\n",
    "\n",
    "df_standard_normalized = standard_normalize(df)\n",
    "show_feature_statistics(df_standard_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.052101Z",
     "start_time": "2021-02-01T12:50:38.041836Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07dddf0794f8e1cfaf87e30dcafd8512",
     "grade": true,
     "grade_id": "test-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(df_standard_normalized.mean(), 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a $k$-nearest-neighbor ($k$-NN) classifier, we can use `sklearn.neighbors.KNeighborsClassifier`. The following fits a $1$-NN classifier to the entire dataset and returns its training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.084674Z",
     "start_time": "2021-02-01T12:50:38.053818Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "X, Y = df[dataset.feature_names], df.target\n",
    "kNN1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "kNN1.fit(X, Y)\n",
    "\n",
    "print('Training accuracy: {:0.3g}'.format(kNN1.score(X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Why is the training accuracy for $1$-NN $100\\%$? Explain according to how 1-NN works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4af86ffab706c5d80300de98c370c2f",
     "grade": true,
     "grade_id": "1-NN",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overly-optimistic performance estimates, the following uses 10-fold cross validations to compute the accuracies of 1-NN trained on datasets with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.322588Z",
     "start_time": "2021-02-01T12:50:38.086329Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "dfs = {\n",
    "    'None': df,\n",
    "    'Min-max': df_minmax_normalized,\n",
    "    'Standard': df_standard_normalized\n",
    "}\n",
    "\n",
    "acc = pd.DataFrame(columns=dfs.keys())\n",
    "for norm in dfs:\n",
    "    acc[norm] = cross_val_score(\n",
    "        kNN1,\n",
    "        dfs[norm].loc[:, lambda df: ~df.columns.isin(['target'])],\n",
    "        # not [dataset.feature_names] since normalized features are renamed\n",
    "        dfs[norm]['target'],\n",
    "        cv=cv)\n",
    "\n",
    "acc.agg(['mean', 'std']).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies computed previously appear to show that normalization improves the performance of 1-NN. However, there are two subtle issues:\n",
    "\n",
    "1. The standard deviations are quite large compared to the difference in performance. The difference in accuracies may be due to the random sampling in obtaining the data and splitting the data.\n",
    "2. The normalization factors are calculated from the entire datasets. Hence, in cross-validation, the normalized data for training a classifier indeed depends on the data for testing the classifier. This can lead to an overly-optimistic estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much we can do to resolve the first issue, other than collecting more data. Repeating the cross-validation with different random seeds do not help as that only smooth out the randomness in splitting, not sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second issue can be easily resolved, however, by computing the normalization factors from training set instead of the entire dataset:\n",
    "- Like the filtered classifier in Weka, `sklearn.pipeline` provides the function `make_pipeline` to combine a filter with a classifier.\n",
    "- `sklearn.preprocessing` provides different filters for normalizing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.425410Z",
     "start_time": "2021-02-01T12:50:38.323742Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "kNN1_standard_normalized = make_pipeline(preprocessing.StandardScaler(), kNN1)\n",
    "acc['Standard'] = cross_val_score(kNN1_standard_normalized, X, Y, cv=cv)\n",
    "acc['Standard'].agg(['mean', 'std']).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance still appears to too large to conclude the improve in performance. Indeed, the accuracy for the pipelined classifier gets even larger than the earlier overly-optimistic accuracy. Like the experimenter interface in Weka, a proper way to compare the performance of different classifiers is to use the [paired t-test](https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Similar to the above cell, correct the accuracies in `acc['Min-max']` to use `preprocessing.MinMaxScaler` as part of a pipeline for the 1-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.545560Z",
     "start_time": "2021-02-01T12:50:38.426545Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bcab84c60d2905e00d9afb365146966",
     "grade": false,
     "grade_id": "min-max",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "acc['Min-max'].agg(['mean', 'std']).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.550933Z",
     "start_time": "2021-02-01T12:50:38.547115Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47dbe5ea58619dabe8bdc62ae9d1e574",
     "grade": true,
     "grade_id": "test-min-max",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `sklearn` does not provide any function to plot the decision regions of a classifier, we provide the function `plot_decision_regions` in a new module `util` defined in `util.py` of the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:38.617405Z",
     "start_time": "2021-02-01T12:50:38.552492Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from util import plot_decision_regions\n",
    "?plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the decision region for a selected pair of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.222608Z",
     "start_time": "2021-02-01T12:50:38.619309Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(normalization=['None', 'Min-max', 'Standard'],\n",
    "          feature1=dataset.feature_names,\n",
    "          feature2=dataset.feature_names,\n",
    "          k=widgets.IntSlider(1, 1, 5, continuous_update=False),\n",
    "          resolution=widgets.IntSlider(1, 1, 4, continuous_update=False))\n",
    "def decision_regions_kNN(normalization,\n",
    "                         feature1=dataset.feature_names[0],\n",
    "                         feature2=dataset.feature_names[1],\n",
    "                         k=1,\n",
    "                         resolution=1):\n",
    "    scaler = {\n",
    "        'Min-max': preprocessing.MinMaxScaler,\n",
    "        'Standard': preprocessing.StandardScaler\n",
    "    }\n",
    "    kNN = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    if normalization != 'None':\n",
    "        kNN = make_pipeline(scaler[normalization](), kNN)\n",
    "    kNN.fit(df[[feature1, feature2]], df.target)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plot_decision_regions(df[[feature1, feature2]],\n",
    "                               df.target,\n",
    "                               kNN,\n",
    "                               N=resolution * 100)\n",
    "    ax.set_title('Decision region for {}-NN'.format(k))\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interact with the widgets to: \n",
    "\n",
    "- Learn the effect on the decision regions/boundaries with different normalizations and choices of $k$.\n",
    "- Learn to draw the decision boundaries for $1$-NN with min-max normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeated computations, `plot_decision_regions` is [a decorated function](https://realpython.com/primer-on-python-decorators/) with its return values memorized/cached. To clear the cached plots, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.233480Z",
     "start_time": "2021-02-01T12:50:39.223816Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_regions.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the following code to plot the decision regions for decision trees. Afterwards, explain whether the decision regions change for different normalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.570095Z",
     "start_time": "2021-02-01T12:50:39.234721Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bfb5dc6304b77cd11dc3f32955aba51",
     "grade": false,
     "grade_id": "DT-region",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@interact(normalization=['None', 'Min-max', 'Standard'],\n",
    "          feature1=dataset.feature_names,\n",
    "          feature2=dataset.feature_names,\n",
    "          resolution=widgets.IntSlider(1, 1, 4, continuous_update=False))\n",
    "def decision_regions_kNN(normalization,\n",
    "                         feature1=dataset.feature_names[0],\n",
    "                         feature2=dataset.feature_names[1],\n",
    "                         resolution=1):\n",
    "    scaler = {\n",
    "        'Min-max': preprocessing.MinMaxScaler,\n",
    "        'Standard': preprocessing.StandardScaler\n",
    "    }\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plot_decision_regions(df[[feature1, feature2]],\n",
    "                               df.target,\n",
    "                               DT,\n",
    "                               N=resolution * 100)\n",
    "    ax.set_title('Decision region for Decision Tree')\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f006f32a0176204e7bba0f6eb5a48ff3",
     "grade": true,
     "grade_id": "DT-decision-regions",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:39.579677Z",
     "start_time": "2021-02-01T12:50:39.571398Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_regions.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although RIPPER is not available in `sklearn`, there is an implementation in [`wittgenstein`](https://pypi.org/project/wittgenstein/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:40.683763Z",
     "start_time": "2021-02-01T12:50:39.580887Z"
    }
   },
   "outputs": [],
   "source": [
    "from wittgenstein import RIPPER\n",
    "\n",
    "ripper = RIPPER(random_state=0)\n",
    "pos_class=dataset.target_names[0]\n",
    "ripper.fit(df, class_feat = 'target', pos_class=pos_class)\n",
    "ripper.out_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the rule for detecting the positive class. To evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:50:44.808240Z",
     "start_time": "2021-02-01T12:50:40.685853Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(ripper, df[dataset.feature_names], df.target == pos_class)\n",
    "print(f'Accuracy: {scores.mean():.3g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge** The `predict` method of `ripper` seems to fail, as the following code does not show a correct decision region. As a challenge, try to identify the issue from the [github repository](https://github.com/imoscovitz/wittgenstein)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T12:51:07.821437Z",
     "start_time": "2021-02-01T12:51:07.121627Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(normalization=['None', 'Min-max', 'Standard'],\n",
    "          feature1=dataset.feature_names,\n",
    "          feature2=dataset.feature_names,\n",
    "          resolution=widgets.IntSlider(1, 1, 4, continuous_update=False))\n",
    "def decision_regions_kNN(normalization,\n",
    "                         feature1=dataset.feature_names[0],\n",
    "                         feature2=dataset.feature_names[1],\n",
    "                         resolution=1):\n",
    "    scaler = {\n",
    "        'Min-max': preprocessing.MinMaxScaler,\n",
    "        'Standard': preprocessing.StandardScaler\n",
    "    }\n",
    "    ripper = RIPPER(random_state=0)\n",
    "    if normalization != 'None':\n",
    "        ripper = make_pipeline(scaler[normalization](), ripper)\n",
    "    X = df[[feature1, feature2]].to_numpy()\n",
    "    Y = df.target == pos_class\n",
    "    ripper.fit(X, Y)\n",
    "    ripper.out_model()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plot_decision_regions(X,\n",
    "                               Y,\n",
    "                               ripper,\n",
    "                               N=resolution * 100)\n",
    "    ax.set_title('Decision region for RIPPER')\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2) \n",
    "    \n",
    "plot_decision_regions.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "beige"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
