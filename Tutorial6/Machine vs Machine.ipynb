{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine vs Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a title=\"Freddycastillo9871, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Terminator_2.png\"><img width=\"512\" alt=\"Terminator 2\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/38/Terminator_2.png\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:14:38.712639Z",
     "start_time": "2021-02-19T10:14:38.581661Z"
    },
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "from weka.core import dataset\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier, Evaluation, SingleClassifierEnhancer\n",
    "from sklearn import ensemble, tree\n",
    "from weka.core.classes import Random\n",
    "from weka.core.classes import complete_classname\n",
    "from sklearn import ensemble\n",
    "from scipy.io import arff\n",
    "import urllib.request\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to build the best machine to classify the image segmentation dataset:\n",
    "- `segment-challenge.arff` for training, and\n",
    "- `segment-test.arff` for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weka provides a KnowledgeFlow interface to flow data through a learning algorithm. The following is a demo for training and testing a J48 decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:14:38.724842Z",
     "start_time": "2021-02-19T10:14:38.714161Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=3f33d95a-c5c2-4893-925b-acd10064acec&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7e941101d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=3f33d95a-c5c2-4893-925b-acd10064acec&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\", height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the other interfaces, the evaluation results can be saved to files as demonstrated below. You may also load an existing template using a button in the toolbar on the top right-hand corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:14:38.735966Z",
     "start_time": "2021-02-19T10:14:38.726496Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=e60913e2-128b-4e40-962d-acd1006b8347&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7e94990c90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=e60913e2-128b-4e40-962d-acd1006b8347&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\", height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the other interfaces, KnowledgeFlow interface can train a classifier incrementally as more and more data becomes available. For more details, refer to the manual [here](https://www.cs.waikato.ac.nz/ml/weka/Witten_et_al_2016_appendix.pdf) and the [video tutorial](https://www.futurelearn.com/info/courses/more-data-mining-with-weka/0/steps/29106) by Witten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Explorer interface, we can flow data through multiple classification algorithms simultaneously as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T10:14:38.747832Z",
     "start_time": "2021-02-19T10:14:38.737927Z"
    },
    "init_cell": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"805\"\n",
       "            height=\"450\"\n",
       "            src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=0d3c11b4-7809-45c7-a208-acd1006ea2f1&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7e94990310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.IFrame(src=\"https://cityuhk-lms.ap.panopto.com/Panopto/Pages/Embed.aspx?id=0d3c11b4-7809-45c7-a208-acd1006ea2f1&autoplay=false&offerviewer=true&showtitle=true&showbrand=false&start=0&interactivity=all\",  height=450, width=805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Follow the video above to add other classifiers: `IBk`, `ZeroR`, `OneR`, `PART`, and `JRIP`. Record their *fractional* accuracies in the dictionary `performance` as follows:\n",
    "\n",
    "```Python\n",
    "performance = {'J48': 0.961728,\n",
    "               'IBk': ___,\n",
    "               'ZeroR': ___,\n",
    "               'OneR': ___,\n",
    "               'PART': ___,\n",
    "               'JRIP': ___}\n",
    "```\n",
    "\n",
    "Use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:14.817913Z",
     "start_time": "2021-02-19T09:52:14.807632Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fdc0f9a1b555aa8fd103a684fe9a4cc",
     "grade": false,
     "grade_id": "kf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:14.829584Z",
     "start_time": "2021-02-19T09:52:14.819821Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9985b8f945cb9e70f0e13f27579bfedd",
     "grade": true,
     "grade_id": "test-kf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the ensemble methods implemented in scikit-learn and Weka to train armies of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following load the data for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:14.970719Z",
     "start_time": "2021-02-19T09:52:14.831945Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_url(url):\n",
    "    ftpstream = urllib.request.urlopen(url)\n",
    "    df = pd.DataFrame(arff.loadarff(io.StringIO(ftpstream.read().decode('utf-8')))[0])\n",
    "    return df.loc[:,lambda df: ~df.columns.isin(['class'])], df['class'].astype(str)\n",
    "\n",
    "weka_data_path = 'https://raw.githubusercontent.com/Waikato/weka-3.8/master/wekadocs/data/'\n",
    "X_train, Y_train = load_url(weka_data_path + 'segment-challenge.arff')\n",
    "X_test, Y_test = load_url(weka_data_path + 'segment-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loads the data for `python-weka-wrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:15.083870Z",
     "start_time": "2021-02-19T09:52:14.973080Z"
    }
   },
   "outputs": [],
   "source": [
    "jvm.start()\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "trainset = loader.load_url(\n",
    "    weka_data_path +\n",
    "    'segment-challenge.arff')  # use load_file to load from file instead\n",
    "trainset.class_is_last()\n",
    "\n",
    "testset = loader.load_url(weka_data_path + 'segment-test.arff')\n",
    "testset.class_is_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple ensemble method is called Bagging (Bootstrap Aggregation), which train different base classifiers by applying one classification algorithm to different bootstrapped datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the following uses the [`sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to train 10 decision trees with maximum depth 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:15.168344Z",
     "start_time": "2021-02-19T09:52:15.085861Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "BAG = ensemble.BaggingClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0)\n",
    "\n",
    "BAG.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {BAG.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T05:31:30.833082Z",
     "start_time": "2021-02-19T05:31:30.365744Z"
    }
   },
   "source": [
    "The ensemble method can be parallelized for both training and classification, by setting additional parameter `n_jobs` (number of jobs to run in parallel) to a number other than 1. To see the effect, execute the following cell and answer `y` to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:34.577254Z",
     "start_time": "2021-02-19T09:52:15.169747Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [y/N] ') == 'y':\n",
    "    for n_jobs in [1, 2, 4, 8]:\n",
    "        BAG.set_params(n_estimators=1000, verbose=1, n_jobs = n_jobs)\n",
    "        BAG.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a diminishing speedup as the number of jobs increases. This is because of both the memory and communication overheads required to parallelize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to see the effect of changing the depth and number of estimators as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:34.582875Z",
     "start_time": "2021-02-19T09:52:34.579007Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "def bagging(n_estimators, max_depth):\n",
    "    BAG = ensemble.BaggingClassifier(\n",
    "        base_estimator=tree.DecisionTreeClassifier(max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=0)\n",
    "    BAG.fit(X_train, Y_train)\n",
    "    return BAG.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run `bag` for different values of `max_depth` and `n_estimators` stored in `max_depth_list` and `n_estimators_list` respectively. The following parallelize the execution using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:41.430822Z",
     "start_time": "2021-02-19T09:52:34.584070Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "if input('execute? [y/N] ') == 'y':\n",
    "    results = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(bagging)(n_estimators, max_depth)\n",
    "        for max_depth in max_depth_list for n_estimators in n_estimators_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the result nicely in a table and a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:46.020264Z",
     "start_time": "2021-02-19T09:52:41.432891Z"
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [y/N] ') == 'y':\n",
    "    BAG_df = pd.DataFrame(\n",
    "    columns=[f'max_depth={max_depth}' for max_depth in max_depth_list],\n",
    "    dtype=float)\n",
    "    BAG_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    BAG_df.loc[:, lambda df: ~df.columns.isin(['n_estimators'])] = np.reshape(\n",
    "        results, (len(n_estimators_list), len(max_depth_list)), order='F')\n",
    "    display.display(BAG_df)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in BAG_df.columns[1:]:\n",
    "        plt.plot(BAG_df['n_estimators'], BAG_df[col], label=col, marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Bagging Decision Trees')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:18:05.446432Z",
     "start_time": "2021-02-19T04:18:05.442861Z"
    }
   },
   "source": [
    "We can also use `GridSearchCV` to tune the parameters for the best model. To tune `n_estimators` by searching for the best value from `n_estimators_list` that maximizes the cross-validated accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:52.313546Z",
     "start_time": "2021-02-19T09:52:46.022046Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid=[{'n_estimators': n_estimators_list}]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    ensemble.BaggingClassifier(\n",
    "        base_estimator=tree.DecisionTreeClassifier(max_depth=10),\n",
    "        random_state=0), \n",
    "    param_grid, verbose=1, n_jobs=4)\n",
    "\n",
    "if input('execute? [y/N] ') == 'y':\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    print(f'Accuracy: {grid_search.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** What happens to the accuracy as `n_estimators` and `max_depth` increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e88504de7fc48187b2c2820a3ed123f",
     "grade": true,
     "grade_id": "bag",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another ensemble method, called random forest, is similar to Bagging decision trees. However, to further diversify the base classifiers, it randomly selects or combines features before building each tree. The following trains a random forest of 10 decision trees with maximum depth 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:52:52.348623Z",
     "start_time": "2021-02-19T09:52:52.315301Z"
    }
   },
   "outputs": [],
   "source": [
    "RF = ensemble.RandomForestClassifier(max_depth=5, \n",
    "                                     n_estimators=10, \n",
    "                                     random_state=0)\n",
    "RF.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {RF.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Bagging, we can also parallelize the training and classification by setting the `n_jobs` parameter. In the above setting, however, the overhead out-weights the benefit, so it is better not to parallelize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `RF_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. Use `random_state = 0`.\n",
    "\n",
    "Please keep your code in the body of the condition `if input('execute? [y/N] ') == 'y':` so we do not re-train the classifiers during auto-grading of your notebook. Re-training will likely take too much time, which will be stopped forcibly by the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:03.008203Z",
     "start_time": "2021-02-19T09:52:52.350167Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adaeefc257e60f79288356006ce0fd01",
     "grade": false,
     "grade_id": "rf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [y/N] ') == 'y':\n",
    "    max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "    RF_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    RF_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(RF_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in RF_df.columns[1:]:\n",
    "        plt.plot(RF_df['n_estimators'],\n",
    "                 RF_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Random forest of different sizes and depths')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a random forest of 10 decision trees with maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:03.235608Z",
     "start_time": "2021-02-19T09:53:03.031638Z"
    }
   },
   "outputs": [],
   "source": [
    "RF = Classifier(classname=\"weka.classifiers.trees.RandomForest\")\n",
    "RF.options = ['-I', '10',\n",
    "              '-depth', '5',\n",
    "              '-S', '1']\n",
    "RF.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(RF, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Repeat the previous exercise but with Weka instead. Use a random seed of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:14.144141Z",
     "start_time": "2021-02-19T09:53:03.237070Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "107a3eefe12b723743e2629485feb8d4",
     "grade": false,
     "grade_id": "rf-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [y/N] ') == 'y':\n",
    "    max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "    RF_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    RF_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(RF_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in RF_df.columns[1:]:\n",
    "        plt.plot(RF_df['n_estimators'],\n",
    "                 RF_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Random forest of different sizes and depths')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** What are the best choices of `n_estimators` and `max_depth`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23969d8ed907bcab86313621f2e0383a",
     "grade": true,
     "grade_id": "rf-best",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train AdaBoost with 10 decision trees of maximum depth 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:14.306021Z",
     "start_time": "2021-02-19T09:53:14.169436Z"
    }
   },
   "outputs": [],
   "source": [
    "ADB = ensemble.AdaBoostClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0)\n",
    "ADB.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {ADB.score(X_test, Y_test):.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Note that, unlike random forest, AdaBoost cannot be parallelized. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5aa2d2cb9571df93fad853043012639",
     "grade": true,
     "grade_id": "adb-parallelize",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the pandas `DataFrame` `ADB_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. Use `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:24.203889Z",
     "start_time": "2021-02-19T09:53:14.307642Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b75330cf3ced04eb07e46ed34c474f77",
     "grade": false,
     "grade_id": "adb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [y/N] ') == 'y':\n",
    "    max_depth_list = [1, 2, 3, 5, 10]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "    ADB_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    ADB_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(ADB_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in ADB_df.columns[1:]:\n",
    "        plt.plot(ADB_df['n_estimators'],\n",
    "                 ADB_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Adaboost of different sizes and depths')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train AdaBoost with 10 decision trees of maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:24.581425Z",
     "start_time": "2021-02-19T09:53:24.225160Z"
    }
   },
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = ['-L', '5']\n",
    "ADB = SingleClassifierEnhancer(classname=\"weka.classifiers.meta.AdaBoostM1\")\n",
    "ADB.options=['-I', '10',\n",
    "             '-S', '1']\n",
    "ADB.classifier = REPTree\n",
    "ADB.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(ADB, testset)\n",
    "print(f'Accuracy {evl.percent_correct/100:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Repeat the previous exercise but with Weka instead. Use a random seed of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T09:53:31.122937Z",
     "start_time": "2021-02-19T09:53:24.583189Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cad0cc56546a6d00aad21beace89806",
     "grade": false,
     "grade_id": "adb-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if input('execute? [y/N] ') == 'y':\n",
    "    max_depth_list = [1, 2, 3, 5, 10]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "    ADB_df = pd.DataFrame(columns=[f'max_depth={max_depth}' for max_depth in max_depth_list], dtype=float)\n",
    "    ADB_df.insert(0, 'n_estimators', n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    display.display(ADB_df.round(4))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in ADB_df.columns[1:]:\n",
    "        plt.plot(ADB_df['n_estimators'],\n",
    "                 ADB_df[col],\n",
    "                 label=col,\n",
    "                 marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Accuracies')\n",
    "    plt.title(r'Adaboost of different sizes and depths')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Which ensemble method is better, Adaboost or random forest? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e676cdf3cbec4928fa23ca7795ce1e09",
     "grade": true,
     "grade_id": "rf-vs-adb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your own classifier to achieve the highest possible accuracies. You may:\n",
    "- choose different classification algorithms or ensemble methods such as Bagging, Stacking, Voting, XGBoost, etc.\n",
    "- tune the hyper-parameters manually or automatically using `GridSearchCV` in `scikit-learn` or `CVParameterSelection` in Weka.\n",
    "\n",
    "Post your model and results on [Canvas](https://canvas.cityu.edu.hk/courses/39808/discussion_topics/306324) to compete with others. If you want to include your code to this notebook, make sure you put it inside the condition `if input('execute? [y/N] ') == 'y':` to avoid re-training your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example using XGBoost with its default parameters.\n",
    "```Python\n",
    "import xgboost\n",
    "XGB = xgboost.XGBClassifier()\n",
    "XGB.fit(X_train, Y_train)\n",
    "print(f'Accuracy: {XGB.score(X_test, Y_test):.4g')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
