{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Quantities for Decision Tree Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T10:52:41.540124Z",
     "start_time": "2021-01-25T10:52:40.539678Z"
    },
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3107fafbdc76cb005359349c7a880d8b",
     "grade": false,
     "grade_id": "init",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import dit\n",
    "from dit.shannon import entropy, conditional_entropy, mutual_information\n",
    "from IPython.display import Math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the [`dit` package](https://dit.readthedocs.io/en/latest/) to compute some basic information quantities used in the decision tree algorithms. A summary of Shannon's information measures and their relationships are given first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are the mathemtical definitions of *entropy* and *mutual information*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "H(Y) &= E\\left[\\log \\tfrac1{P_{Y}(Y)}\\right] && \\text{entropy of $Y$}\\\\\n",
    "H(Y|X) &= E\\left[\\log \\tfrac1{P_{Y|X}(Y|X)}\\right] && \\text{conditional entropy of $Y$ given $X$}\\\\\n",
    "I(X;Y) &= E\\left[\\log \\tfrac{P_{Y|X}(Y|X)}{P_Y(Y)}\\right] && \\text{mutual information of $X$ and $Y$}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These information quantities can be related using a *Venn Diagram*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a title=\"KonradVoelkel, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Entropy-mutual-information-relative-entropy-relation-diagram.svg\"><img width=\"512\" alt=\"Entropy-mutual-information-relative-entropy-relation-diagram\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "H(X,Y)&=H(X)+H(Y|X) && \\text{chain rule of entropy}\\\\\n",
    "&=H(Y)+H(X|Y)\\\\\n",
    "I(X;Y)&=H(Y)-H(Y|X) && \\text{mutual information in terms of entropies}\\\\\n",
    "&=H(X)+H(Y)-H(X,Y)\\\\\n",
    "&=H(X)-H(X|Y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following distribution:\n",
    "\\begin{align}\n",
    "p_k=\\begin{cases}\n",
    "\\frac12 & k=0\\\\\n",
    "\\frac14 & k=1,2\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:42:57.466030Z",
     "start_time": "2021-01-22T07:42:57.361131Z"
    }
   },
   "outputs": [],
   "source": [
    "p = dit.Distribution(['0', '1', '2', '3'], [1/2, 1/4, 1/4, 0])\n",
    "\n",
    "plt.stem(p.outcomes,p.pmf,use_line_collection=True)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(r'$p_k$')\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:44:00.199245Z",
     "start_time": "2021-01-22T07:44:00.192138Z"
    }
   },
   "outputs": [],
   "source": [
    "Math(f'h(p_1,p_2,\\dots)={entropy(p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the dataset $D$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|X1|X2|X3|X4|Y|\n",
    "|:---:|:---:|:---:|:---:|:-:|\n",
    "|0    |0    |0    |00   |0  |\n",
    "|0    |0    |0    |00   |0  |\n",
    "|0    |0    |1    |01   |1  |\n",
    "|1    |0    |1    |11   |1  |\n",
    "|0    |1    |0    |00   |2  |\n",
    "|1    |1    |0    |10   |2  |\n",
    "|1    |1    |1    |11   |3  |\n",
    "|1    |1    |1    |11   |3  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to determine which attribute is more informative?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a uniform distribution over the instances in $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:51:58.347625Z",
     "start_time": "2021-01-22T07:51:58.336703Z"
    }
   },
   "outputs": [],
   "source": [
    "d = dit.uniform([('0','0','0','00','0'),\n",
    "                 ('0','0','0','00','0'),\n",
    "                 ('0','0','1','01','1'),\n",
    "                 ('1','0','1','11','1'),\n",
    "                 ('0','1','0','00','2'),\n",
    "                 ('1','1','0','10','2'),\n",
    "                 ('1','1','1','11','3'),\n",
    "                 ('1','1','1','11','3')])\n",
    "d.set_rv_names(('X1','X2','X3','X4','Y'))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate $\\text{Info}(D)$ and $\\text{Info}_{X_i}(D)$ for $i=\\{1,2,3,4\\}$ as the entropy $H(Y)$ and conditional entropies $H(Y|X_i)$'s respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:55:25.328775Z",
     "start_time": "2021-01-22T07:55:25.314356Z"
    }
   },
   "outputs": [],
   "source": [
    "InfoD = entropy(d,['Y'])\n",
    "InfoX1D = conditional_entropy(d,['Y'],['X1'])\n",
    "InfoX2D = conditional_entropy(d,['Y'],['X2'])\n",
    "InfoX3D = conditional_entropy(d,['Y'],['X3'])\n",
    "InfoX4D = conditional_entropy(d,['Y'],['X4'])\n",
    "\n",
    "Math(r'''\n",
    "\\begin{{aligned}}\n",
    "\\text{{Info}}(D)&={}\\\\\n",
    "\\text{{Info}}_{{X_1}}(D)&={:.3g}\\\\\n",
    "\\text{{Info}}_{{X_2}}(D)&={:.3g}\\\\\n",
    "\\text{{Info}}_{{X_3}}(D)&={:.3g}\\\\\n",
    "\\text{{Info}}_{{X_4}}(D)&={:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "'''.format(InfoD,InfoX1D,InfoX2D,InfoX3D,InfoX4D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information gain $\\text{Gain}_{X_i}(D)$ can be calculated as the mutual information $I(X_i;Y):=H(Y)-H(Y|X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:55:00.916527Z",
     "start_time": "2021-01-22T07:55:00.911499Z"
    }
   },
   "outputs": [],
   "source": [
    "GainX1D = mutual_information(d,['X1'],['Y'])\n",
    "GainX2D = mutual_information(d,['X2'],['Y'])\n",
    "GainX3D = mutual_information(d,['X3'],['Y'])\n",
    "GainX4D = mutual_information(d,['X4'],['Y'])\n",
    "\n",
    "Math(r'''\n",
    "\\begin{{aligned}}\n",
    "\\text{{Gain}}_{{X_1}}(D)&={:.3g}\\\\\n",
    "\\text{{Gain}}_{{X_2}}(D)&={:.3g}\\\\\n",
    "\\text{{Gain}}_{{X_3}}(D)&={:.3g}\\\\\n",
    "\\text{{Gain}}_{{X_4}}(D)&={:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "'''.format(GainX1D,GainX2D,GainX3D,GainX4D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Which attribute gives the highest information gain? Should we choose it as the splitting attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e636a548f742243280ce43d03d91000",
     "grade": true,
     "grade_id": "infogain",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize information gain properly, we first calculate $\\text{SplitInfo}_{X_i}(D)$ as $H(X_i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:56:50.235255Z",
     "start_time": "2021-01-22T07:56:50.221835Z"
    }
   },
   "outputs": [],
   "source": [
    "SplitInfoX1D = entropy(d,['X1'])\n",
    "SplitInfoX2D = entropy(d,['X2'])\n",
    "SplitInfoX3D = entropy(d,['X3'])\n",
    "SplitInfoX4D = entropy(d,['X4'])\n",
    "\n",
    "Math(r'''\n",
    "\\begin{{aligned}}\n",
    "\\text{{SplitInfo}}_{{X_1}}(D)&={:.3g}\\\\\n",
    "\\text{{SplitInfo}}_{{X_2}}(D)&={:.3g}\\\\\n",
    "\\text{{SplitInfo}}_{{X_3}}(D)&={:.3g}\\\\\n",
    "\\text{{SplitInfo}}_{{X_4}}(D)&={:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "'''.format(SplitInfoX1D,SplitInfoX2D,SplitInfoX3D,SplitInfoX4D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to calculate the information gain ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T07:58:00.250276Z",
     "start_time": "2021-01-22T07:58:00.242843Z"
    }
   },
   "outputs": [],
   "source": [
    "Math(r'''\n",
    "\\begin{{aligned}}\n",
    "\\frac{{\\text{{Gain}}_{{X_1}}(D)}}{{\\text{{SplitInfo}}_{{X_1}}(D)}}&={:.3g}\\\\\n",
    "\\frac{{\\text{{Gain}}_{{X_2}}(D)}}{{\\text{{SplitInfo}}_{{X_2}}(D)}}&={:.3g}\\\\\n",
    "\\frac{{\\text{{Gain}}_{{X_3}}(D)}}{{\\text{{SplitInfo}}_{{X_3}}(D)}}&={:.3g}\\\\\n",
    "\\frac{{\\text{{Gain}}_{{X_4}}(D)}}{{\\text{{SplitInfo}}_{{X_4}}(D)}}&={:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "'''.format(GainX1D/SplitInfoX1D,GainX2D/SplitInfoX2D,GainX3D/SplitInfoX3D,GainX4D/SplitInfoX4D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Is $X_4$ a good splitting attribute? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e9d188661e23c703a0fd1cff8f3bd33",
     "grade": true,
     "grade_id": "ratio",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "beige"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
